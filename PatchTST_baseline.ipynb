{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch import Tensor\n",
    "from typing import Callable, Optional\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  date   HUFL   HULL   MUFL   MULL   LUFL   LULL         OT\n",
      "0  2016-07-01 00:00:00  5.827  2.009  1.599  0.462  4.203  1.340  30.531000\n",
      "1  2016-07-01 01:00:00  5.693  2.076  1.492  0.426  4.142  1.371  27.787001\n",
      "2  2016-07-01 02:00:00  5.157  1.741  1.279  0.355  3.777  1.218  27.787001\n",
      "3  2016-07-01 03:00:00  5.090  1.942  1.279  0.391  3.807  1.279  25.044001\n",
      "4  2016-07-01 04:00:00  5.358  1.942  1.492  0.462  3.868  1.279  21.948000\n"
     ]
    }
   ],
   "source": [
    "df_ETTh1 = pd.read_csv(\"/scratch/ab10445/hpml/PatchTST/dataset/ETT-small/ETTh1.csv\")\n",
    "print(df_ETTh1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17420, 7), (17420, 8), 8640)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ETTh1.iloc[:,1:].shape, df_ETTh1.shape, 12 * 30 * 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset=\"ETTh1\", mode=\"train\", scale=True, seq_len=336, pred_len=96):\n",
    "        super().__init__()\n",
    "        df = pd.read_csv(\"/scratch/ab10445/hpml/PatchTST/dataset/ETT-small/ETTh1.csv\".format(dataset))\n",
    "        x_y = df.iloc[:,1:]\n",
    "        time_stamp = df.iloc[:,0]\n",
    "\n",
    "        assert mode in ['train', 'test', 'val']\n",
    "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
    "        self.set_type = type_map[mode]\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "        border1s = [0, 12 * 30 * 24 - self.seq_len, 12 * 30 * 24 + 4 * 30 * 24 - self.seq_len]\n",
    "        border2s = [12 * 30 * 24, 12 * 30 * 24 + 4 * 30 * 24, 12 * 30 * 24 + 8 * 30 * 24]\n",
    "        border1 = border1s[self.set_type]\n",
    "        border2 = border2s[self.set_type]\n",
    "\n",
    "        if scale:\n",
    "            train_x_y = x_y.iloc[border1s[0]: border2s[0]]\n",
    "            self.ss = StandardScaler()\n",
    "            self.ss.fit(train_x_y.to_numpy(dtype=np.float32))\n",
    "            x_y = self.ss.transform(x_y.to_numpy(dtype=np.float32))\n",
    "        else:\n",
    "            x_y = x_y.to_numpy(dtype=np.float32)\n",
    "        \n",
    "        time_stamp = time_stamp.to_numpy()     \n",
    "        \n",
    "        self.data_x = x_y[border1: border2, :]\n",
    "        self.data_y = x_y[border1: border2, -1]\n",
    "\n",
    "        self.data_stamp = time_stamp[border1: border2]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end\n",
    "        r_end = r_begin + self.pred_len\n",
    "\n",
    "        seq_x = self.data_x[s_begin:s_end]\n",
    "        seq_y = self.data_y[r_begin:r_end]\n",
    "        return seq_x, seq_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x) - self.seq_len - self.pred_len + 1\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return self.ss.inverse_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Revin.py\n",
    "class RevIN(torch.nn.Module):\n",
    "    def __init__(self, num_features: int, eps=1e-5, affine=True, subtract_last=False):\n",
    "        \"\"\"\n",
    "        :param num_features: the number of features or channels\n",
    "        :param eps: a value added for numerical stability\n",
    "        :param affine: if True, RevIN has learnable affine parameters\n",
    "        \"\"\"\n",
    "        super(RevIN, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "        self.subtract_last = subtract_last\n",
    "        if self.affine:\n",
    "            self._init_params()\n",
    "\n",
    "    def forward(self, x, mode:str):\n",
    "        if mode == 'norm':\n",
    "            self._get_statistics(x)\n",
    "            x = self._normalize(x)\n",
    "        elif mode == 'denorm':\n",
    "            x = self._denormalize(x)\n",
    "        else: raise NotImplementedError\n",
    "        return x\n",
    "\n",
    "    def _init_params(self):\n",
    "        # initialize RevIN params: (C,)\n",
    "        self.affine_weight = nn.Parameter(torch.ones(self.num_features))\n",
    "        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))\n",
    "\n",
    "    def _get_statistics(self, x):\n",
    "        dim2reduce = tuple(range(1, x.ndim-1))\n",
    "        if self.subtract_last:\n",
    "            self.last = x[:,-1,:].unsqueeze(1)\n",
    "        else:\n",
    "            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
    "        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n",
    "\n",
    "    def _normalize(self, x):\n",
    "        if self.subtract_last:\n",
    "            x = x - self.last\n",
    "        else:\n",
    "            x = x - self.mean\n",
    "        x = x / self.stdev\n",
    "        if self.affine:\n",
    "            # print(f'x.mT.shape: {x.mT.shape} | self.affine_weight: {self.affine_weight.shape}')\n",
    "            x = x * self.affine_weight + self.affine_bias\n",
    "            # print(f'x.shape: {x.shape} | self.affine_weight: {self.affine_bias.shape}')\n",
    "            # x = x + self.affine_bias\n",
    "        return x.mT\n",
    "\n",
    "    def _denormalize(self, x):\n",
    "        if self.affine:\n",
    "            x = x - self.affine_bias\n",
    "            x = x / (self.affine_weight + self.eps*self.eps)\n",
    "        x = x * self.stdev\n",
    "        if self.subtract_last:\n",
    "            x = x + self.last\n",
    "        else:\n",
    "            x = x + self.mean\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PatchTST_layers.py\n",
    "class Transpose(torch.nn.Module):\n",
    "    def __init__(self, *dims, contiguous=False): \n",
    "        super().__init__()\n",
    "        self.dims, self.contiguous = dims, contiguous\n",
    "    def forward(self, x):\n",
    "        if self.contiguous: return x.transpose(*self.dims).contiguous()\n",
    "        else: return x.transpose(*self.dims)\n",
    "\n",
    "def positional_encoding(pe, learn_pe, q_len, d_model):\n",
    "    # Positional encoding\n",
    "    if pe == None:\n",
    "        W_pos = torch.empty((q_len, d_model)) # pe = None and learn_pe = False can be used to measure impact of pe\n",
    "        nn.init.uniform_(W_pos, -0.02, 0.02)\n",
    "        learn_pe = False\n",
    "    elif pe == 'zero':\n",
    "        W_pos = torch.empty((q_len, 1))\n",
    "        nn.init.uniform_(W_pos, -0.02, 0.02)\n",
    "    elif pe == 'zeros':\n",
    "        W_pos = torch.empty((q_len, d_model))\n",
    "        nn.init.uniform_(W_pos, -0.02, 0.02)\n",
    "    elif pe == 'normal' or pe == 'gauss':\n",
    "        W_pos = torch.zeros((q_len, 1))\n",
    "        torch.nn.init.normal_(W_pos, mean=0.0, std=0.1)\n",
    "    elif pe == 'uniform':\n",
    "        W_pos = torch.zeros((q_len, 1))\n",
    "        nn.init.uniform_(W_pos, a=0.0, b=0.1)\n",
    "    elif pe == 'lin1d': W_pos = Coord1dPosEncoding(q_len, exponential=False, normalize=True)\n",
    "    elif pe == 'exp1d': W_pos = Coord1dPosEncoding(q_len, exponential=True, normalize=True)\n",
    "    elif pe == 'lin2d': W_pos = Coord2dPosEncoding(q_len, d_model, exponential=False, normalize=True)\n",
    "    elif pe == 'exp2d': W_pos = Coord2dPosEncoding(q_len, d_model, exponential=True, normalize=True)\n",
    "    elif pe == 'sincos': W_pos = PositionalEncoding(q_len, d_model, normalize=True)\n",
    "    else: raise ValueError(f\"{pe} is not a valid pe (positional encoder. Available types: 'gauss'=='normal', \\\n",
    "        'zeros', 'zero', uniform', 'lin1d', 'exp1d', 'lin2d', 'exp2d', 'sincos', None.)\")\n",
    "    return nn.Parameter(W_pos, requires_grad=learn_pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PatchTST_backbone.py\n",
    "class TSTiEncoder(torch.nn.Module):  #i means channel-independent\n",
    "    def __init__(self, c_in, patch_num, patch_len, max_seq_len=1024,\n",
    "                 n_layers=3, d_model=128, n_heads=16, d_k=None, d_v=None,\n",
    "                 d_ff=128, norm='BatchNorm', attn_dropout=0., dropout=0., act=\"gelu\", store_attn=False,\n",
    "                 key_padding_mask='auto', padding_var=None, attn_mask=None, res_attention=True, pre_norm=False,\n",
    "                 pe='zeros', learn_pe=True, verbose=False, **kwargs):\n",
    "                 #act=\"gelu\" \n",
    "        \n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_num = patch_num\n",
    "        self.patch_len = patch_len\n",
    "        \n",
    "        # Input encoding\n",
    "        q_len = patch_num\n",
    "        self.W_P = nn.Linear(patch_len, d_model)        # Eq 1: projection of feature vectors onto a d-dim vector space\n",
    "        self.seq_len = q_len\n",
    "\n",
    "        # Positional encoding\n",
    "        self.W_pos = positional_encoding(pe, learn_pe, q_len, d_model)\n",
    "\n",
    "        # Residual dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = TSTEncoder(q_len, d_model, n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm, attn_dropout=attn_dropout, dropout=dropout,\n",
    "                                   pre_norm=pre_norm, activation=act, res_attention=res_attention, n_layers=n_layers, store_attn=store_attn)\n",
    "\n",
    "        \n",
    "    def forward(self, x) -> Tensor:                                              # x: [bs x nvars x patch_len x patch_num]\n",
    "        \n",
    "        n_vars = x.shape[1]\n",
    "        # Input encoding\n",
    "        x = x.permute(0,1,3,2)                                                   # x: [bs x nvars x patch_num x patch_len]\n",
    "        x = self.W_P(x)                                                          # x: [bs x nvars x patch_num x d_model]\n",
    "\n",
    "        u = torch.reshape(x, (x.shape[0]*x.shape[1],x.shape[2],x.shape[3]))      # u: [bs * nvars x patch_num x d_model]\n",
    "        u = self.dropout(u + self.W_pos)                                         # u: [bs * nvars x patch_num x d_model]\n",
    "\n",
    "        # Encoder\n",
    "        z = self.encoder(u)                                                      # z: [bs * nvars x patch_num x d_model]\n",
    "        z = torch.reshape(z, (-1,n_vars,z.shape[-2],z.shape[-1]))                # z: [bs x nvars x patch_num x d_model]\n",
    "        z = z.permute(0,1,3,2)                                                   # z: [bs x nvars x d_model x patch_num]\n",
    "        \n",
    "        return z         \n",
    "            \n",
    "    \n",
    "# Cell\n",
    "class TSTEncoder(torch.nn.Module):\n",
    "    def __init__(self, q_len, d_model, n_heads, d_k=None, d_v=None, d_ff=None, \n",
    "                        norm='BatchNorm', attn_dropout=0., dropout=0., activation='gelu',\n",
    "                        res_attention=False, n_layers=1, pre_norm=False, store_attn=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([TSTEncoderLayer(q_len, d_model, n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm,\n",
    "                                                      attn_dropout=attn_dropout, dropout=dropout,\n",
    "                                                      activation=activation, res_attention=res_attention,\n",
    "                                                      pre_norm=pre_norm, store_attn=store_attn) for i in range(n_layers)])\n",
    "        self.res_attention = res_attention\n",
    "\n",
    "    def forward(self, src:Tensor, key_padding_mask:Optional[Tensor]=None, attn_mask:Optional[Tensor]=None):\n",
    "        output = src\n",
    "        scores = None\n",
    "        if self.res_attention:\n",
    "            for mod in self.layers: output, scores = mod(output, prev=scores, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "            return output\n",
    "        else:\n",
    "            for mod in self.layers: output = mod(output, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "            return output\n",
    "\n",
    "\n",
    "\n",
    "class TSTEncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, q_len, d_model, n_heads, d_k=None, d_v=None, d_ff=128, store_attn=False,\n",
    "                 norm='BatchNorm', attn_dropout=0, dropout=0., bias=True, activation=\"gelu\", res_attention=False, pre_norm=False):\n",
    "        super().__init__()\n",
    "        assert not d_model%n_heads, f\"d_model ({d_model}) must be divisible by n_heads ({n_heads})\"\n",
    "        d_k = d_model // n_heads if d_k is None else d_k\n",
    "        d_v = d_model // n_heads if d_v is None else d_v\n",
    "\n",
    "        # Multi-Head attention\n",
    "        self.res_attention = res_attention\n",
    "        self.self_attn = _MultiheadAttention(d_model, n_heads, d_k, d_v, attn_dropout=attn_dropout, proj_dropout=dropout, res_attention=res_attention)\n",
    "\n",
    "        # Add & Norm\n",
    "        self.dropout_attn = nn.Dropout(dropout)\n",
    "        if \"batch\" in norm.lower():\n",
    "            self.norm_attn = nn.Sequential(Transpose(1,2), nn.BatchNorm1d(d_model), Transpose(1,2))\n",
    "        else:\n",
    "            self.norm_attn = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Position-wise Feed-Forward\n",
    "        self.ff = nn.Sequential(nn.Linear(d_model, d_ff, bias=bias),\n",
    "                                torch.nn.GELU(),\n",
    "                                nn.Dropout(dropout),\n",
    "                                nn.Linear(d_ff, d_model, bias=bias))\n",
    "\n",
    "        # Add & Norm\n",
    "        self.dropout_ffn = nn.Dropout(dropout)\n",
    "        if \"batch\" in norm.lower():\n",
    "            self.norm_ffn = nn.Sequential(Transpose(1,2), nn.BatchNorm1d(d_model), Transpose(1,2))\n",
    "        else:\n",
    "            self.norm_ffn = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.pre_norm = pre_norm\n",
    "        self.store_attn = store_attn\n",
    "\n",
    "\n",
    "    def forward(self, src:Tensor, prev:Optional[Tensor]=None, key_padding_mask:Optional[Tensor]=None, attn_mask:Optional[Tensor]=None) -> Tensor:\n",
    "\n",
    "        # Multi-Head attention sublayer\n",
    "        if self.pre_norm:\n",
    "            src = self.norm_attn(src)\n",
    "        ## Multi-Head attention\n",
    "        if self.res_attention:\n",
    "            src2, attn, scores = self.self_attn(src, src, src, prev, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "        else:\n",
    "            src2, attn = self.self_attn(src, src, src, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "        if self.store_attn:\n",
    "            self.attn = attn\n",
    "        ## Add & Norm\n",
    "        src = src + self.dropout_attn(src2) # Add: residual connection with residual dropout\n",
    "        if not self.pre_norm:\n",
    "            src = self.norm_attn(src)\n",
    "\n",
    "        # Feed-forward sublayer\n",
    "        if self.pre_norm:\n",
    "            src = self.norm_ffn(src)\n",
    "        ## Position-wise Feed-Forward\n",
    "        src2 = self.ff(src)\n",
    "        ## Add & Norm\n",
    "        src = src + self.dropout_ffn(src2) # Add: residual connection with residual dropout\n",
    "        if not self.pre_norm:\n",
    "            src = self.norm_ffn(src)\n",
    "\n",
    "        if self.res_attention:\n",
    "            return src, scores\n",
    "        else:\n",
    "            return src\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _MultiheadAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_k=None, d_v=None, res_attention=False, attn_dropout=0., proj_dropout=0., qkv_bias=True, lsa=False):\n",
    "        \"\"\"Multi Head Attention Layer\n",
    "        Input shape:\n",
    "            Q:       [batch_size (bs) x max_q_len x d_model]\n",
    "            K, V:    [batch_size (bs) x q_len x d_model]\n",
    "            mask:    [q_len x q_len]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        d_k = d_model // n_heads if d_k is None else d_k\n",
    "        d_v = d_model // n_heads if d_v is None else d_v\n",
    "\n",
    "        self.n_heads, self.d_k, self.d_v = n_heads, d_k, d_v\n",
    "\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=qkv_bias)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=qkv_bias)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=qkv_bias)\n",
    "\n",
    "        # Scaled Dot-Product Attention (multiple heads)\n",
    "        self.res_attention = res_attention\n",
    "        self.sdp_attn = _ScaledDotProductAttention(d_model, n_heads, attn_dropout=attn_dropout, res_attention=self.res_attention, lsa=lsa)\n",
    "\n",
    "        # Poject output\n",
    "        self.to_out = nn.Sequential(nn.Linear(n_heads * d_v, d_model), nn.Dropout(proj_dropout))\n",
    "\n",
    "\n",
    "    def forward(self, Q:Tensor, K:Optional[Tensor]=None, V:Optional[Tensor]=None, prev:Optional[Tensor]=None,\n",
    "                key_padding_mask:Optional[Tensor]=None, attn_mask:Optional[Tensor]=None):\n",
    "\n",
    "        bs = Q.size(0)\n",
    "        if K is None: K = Q\n",
    "        if V is None: V = Q\n",
    "\n",
    "        # Linear (+ split in multiple heads)\n",
    "        q_s = self.W_Q(Q).view(bs, -1, self.n_heads, self.d_k).transpose(1,2)       # q_s    : [bs x n_heads x max_q_len x d_k]\n",
    "        k_s = self.W_K(K).view(bs, -1, self.n_heads, self.d_k).permute(0,2,3,1)     # k_s    : [bs x n_heads x d_k x q_len] - transpose(1,2) + transpose(2,3)\n",
    "        v_s = self.W_V(V).view(bs, -1, self.n_heads, self.d_v).transpose(1,2)       # v_s    : [bs x n_heads x q_len x d_v]\n",
    "\n",
    "        # Apply Scaled Dot-Product Attention (multiple heads)\n",
    "        if self.res_attention:\n",
    "            output, attn_weights, attn_scores = self.sdp_attn(q_s, k_s, v_s, prev=prev, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "        else:\n",
    "            output, attn_weights = self.sdp_attn(q_s, k_s, v_s, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "        # output: [bs x n_heads x q_len x d_v], attn: [bs x n_heads x q_len x q_len], scores: [bs x n_heads x max_q_len x q_len]\n",
    "\n",
    "        # back to the original inputs dimensions\n",
    "        output = output.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * self.d_v) # output: [bs x q_len x n_heads * d_v]\n",
    "        output = self.to_out(output)\n",
    "\n",
    "        if self.res_attention: return output, attn_weights, attn_scores\n",
    "        else: return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _ScaledDotProductAttention(torch.nn.Module):\n",
    "    r\"\"\"Scaled Dot-Product Attention module (Attention is all you need by Vaswani et al., 2017) with optional residual attention from previous layer\n",
    "    (Realformer: Transformer likes residual attention by He et al, 2020) and locality self sttention (Vision Transformer for Small-Size Datasets\n",
    "    by Lee et al, 2021)\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, n_heads, attn_dropout=0., res_attention=False, lsa=False):\n",
    "        super().__init__()\n",
    "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
    "        self.res_attention = res_attention\n",
    "        head_dim = d_model // n_heads\n",
    "        self.scale = nn.Parameter(torch.tensor(head_dim ** -0.5), requires_grad=lsa)\n",
    "        self.lsa = lsa\n",
    "\n",
    "    def forward(self, q:Tensor, k:Tensor, v:Tensor, prev:Optional[Tensor]=None, key_padding_mask:Optional[Tensor]=None, attn_mask:Optional[Tensor]=None):\n",
    "        '''\n",
    "        Input shape:\n",
    "            q               : [bs x n_heads x max_q_len x d_k]\n",
    "            k               : [bs x n_heads x d_k x seq_len]\n",
    "            v               : [bs x n_heads x seq_len x d_v]\n",
    "            prev            : [bs x n_heads x q_len x seq_len]\n",
    "            key_padding_mask: [bs x seq_len]\n",
    "            attn_mask       : [1 x seq_len x seq_len]\n",
    "        Output shape:\n",
    "            output:  [bs x n_heads x q_len x d_v]\n",
    "            attn   : [bs x n_heads x q_len x seq_len]\n",
    "            scores : [bs x n_heads x q_len x seq_len]\n",
    "        '''\n",
    "\n",
    "        # Scaled MatMul (q, k) - similarity scores for all pairs of positions in an input sequence\n",
    "        attn_scores = torch.matmul(q, k) * self.scale      # attn_scores : [bs x n_heads x max_q_len x q_len]\n",
    "\n",
    "        # Add pre-softmax attention scores from the previous layer (optional)\n",
    "        if prev is not None: attn_scores = attn_scores + prev\n",
    "\n",
    "        # Attention mask (optional)\n",
    "        if attn_mask is not None:                                     # attn_mask with shape [q_len x seq_len] - only used when q_len == seq_len\n",
    "            if attn_mask.dtype == torch.bool:\n",
    "                attn_scores.masked_fill_(attn_mask, -np.inf)\n",
    "            else:\n",
    "                attn_scores += attn_mask\n",
    "\n",
    "        # Key padding mask (optional)\n",
    "        if key_padding_mask is not None:                              # mask with shape [bs x q_len] (only when max_w_len == q_len)\n",
    "            attn_scores.masked_fill_(key_padding_mask.unsqueeze(1).unsqueeze(2), -np.inf)\n",
    "\n",
    "        # normalize the attention weights\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)                 # attn_weights   : [bs x n_heads x max_q_len x q_len]\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "\n",
    "        # compute the new values given the attention weights\n",
    "        output = torch.matmul(attn_weights, v)                        # output: [bs x n_heads x max_q_len x d_v]\n",
    "\n",
    "        if self.res_attention: return output, attn_weights, attn_scores\n",
    "        else: return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten_Head(torch.nn.Module):\n",
    "    def __init__(self, individual, n_vars, nf, target_window, head_dropout=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.individual = individual\n",
    "        self.n_vars = n_vars\n",
    "        \n",
    "        if self.individual:\n",
    "            self.linears = nn.ModuleList()\n",
    "            self.dropouts = nn.ModuleList()\n",
    "            self.flattens = nn.ModuleList()\n",
    "            for i in range(self.n_vars):\n",
    "                self.flattens.append(nn.Flatten(start_dim=-2)) #flattens last 2\n",
    "                self.linears.append(nn.Linear(nf, target_window))\n",
    "                self.dropouts.append(nn.Dropout(head_dropout))\n",
    "        else:\n",
    "            self.flatten = nn.Flatten(start_dim=-2)\n",
    "            self.linear = nn.Linear(nf, target_window)\n",
    "            self.dropout = nn.Dropout(head_dropout)\n",
    "            \n",
    "    def forward(self, x):                                 # x: [bs x nvars x d_model x patch_num]\n",
    "        if self.individual:\n",
    "            x_out = []\n",
    "            for i in range(self.n_vars):\n",
    "                z = self.flattens[i](x[:,i,:,:])          # z: [bs x d_model * patch_num]\n",
    "                z = self.linears[i](z)                    # z: [bs x target_window]\n",
    "                z = self.dropouts[i](z)\n",
    "                x_out.append(z)\n",
    "            x = torch.stack(x_out, dim=1)                 # x: [bs x nvars x target_window]\n",
    "        else:\n",
    "            x = self.flatten(x)\n",
    "            x = self.linear(x)\n",
    "            x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "#Implementation of PatchTST\n",
    "def Coord2dPosEncoding(q_len, d_model, exponential=False, normalize=True, eps=1e-3, verbose=False):\n",
    "    x = .5 if exponential else 1\n",
    "    i = 0\n",
    "    for i in range(100):\n",
    "        cpe = 2 * (torch.linspace(0, 1, q_len).reshape(-1, 1) ** x) * (torch.linspace(0, 1, d_model).reshape(1, -1) ** x) - 1\n",
    "        pv(f'{i:4.0f}  {x:5.3f}  {cpe.mean():+6.3f}', verbose)\n",
    "        if abs(cpe.mean()) <= eps: break\n",
    "        elif cpe.mean() > eps: x += .001\n",
    "        else: x -= .001\n",
    "        i += 1\n",
    "    if normalize:\n",
    "        cpe = cpe - cpe.mean()\n",
    "        cpe = cpe / (cpe.std() * 10)\n",
    "    return cpe\n",
    "\n",
    "def Coord1dPosEncoding(q_len, exponential=False, normalize=True):\n",
    "    cpe = (2 * (torch.linspace(0, 1, q_len).reshape(-1, 1)**(.5 if exponential else 1)) - 1)\n",
    "    if normalize:\n",
    "        cpe = cpe - cpe.mean()\n",
    "        cpe = cpe / (cpe.std() * 10)\n",
    "    return cpe\n",
    "\n",
    "class PatchTST(torch.nn.Module):\n",
    "    def __init__(self, c_in, context_window, target_window, patch_len, stride, max_seq_len:Optional[int]=1024, \n",
    "                 n_layers=3, d_model=128, n_heads=4, d_k:Optional[int]=None, d_v:Optional[int]=None,\n",
    "                 d_ff:int=128, norm:str='BatchNorm', attn_dropout:float=0., dropout:float=0.3, act:str=\"gelu\", key_padding_mask:bool='auto',\n",
    "                 padding_var:Optional[int]=None, attn_mask:Optional[Tensor]=None, res_attention:bool=True, pre_norm:bool=False, store_attn:bool=False,\n",
    "                 pe:str='zeros', learn_pe:bool=True, fc_dropout:float=0.3, head_dropout = 0, padding_patch = None,\n",
    "                 pretrain_head:bool=False, head_type = 'flatten', individual = False, revin = True, affine = True, subtract_last = False,\n",
    "                 verbose:bool=False, **kwargs):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # RevIn\n",
    "        self.revin = revin\n",
    "        if self.revin: self.revin_layer = RevIN(c_in, affine=affine, subtract_last=subtract_last)\n",
    "        \n",
    "        # Patching\n",
    "        self.patch_len = patch_len\n",
    "        self.stride = stride\n",
    "        self.padding_patch = padding_patch\n",
    "        patch_num = int((context_window - patch_len)/stride + 1)\n",
    "        if padding_patch == 'end': # can be modified to general case\n",
    "            self.padding_patch_layer = nn.ReplicationPad1d((0, stride)) \n",
    "            patch_num += 1\n",
    "        \n",
    "        # Backbone \n",
    "        self.backbone = TSTiEncoder(c_in, patch_num=patch_num, patch_len=patch_len, max_seq_len=max_seq_len,\n",
    "                                n_layers=n_layers, d_model=d_model, n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff,\n",
    "                                attn_dropout=attn_dropout, dropout=dropout, act=act, key_padding_mask=key_padding_mask, padding_var=padding_var,\n",
    "                                attn_mask=attn_mask, res_attention=res_attention, pre_norm=pre_norm, store_attn=store_attn,\n",
    "                                pe=pe, learn_pe=learn_pe, verbose=verbose, **kwargs)\n",
    "\n",
    "        # Head\n",
    "        self.head_nf = d_model * patch_num\n",
    "        self.n_vars = c_in\n",
    "        self.pretrain_head = pretrain_head\n",
    "        self.head_type = head_type\n",
    "        self.individual = individual\n",
    "\n",
    "        if self.pretrain_head: \n",
    "            self.head = self.create_pretrain_head(self.head_nf, c_in, fc_dropout) # custom head passed as a partial func with all its kwargs\n",
    "        elif head_type == 'flatten': \n",
    "            self.head = Flatten_Head(self.individual, self.n_vars, self.head_nf, target_window, head_dropout=head_dropout)\n",
    "        \n",
    "    \n",
    "    def forward(self, z):                                                                   # z: [bs x nvars x seq_len]\n",
    "        # norm\n",
    "        if self.revin: \n",
    "            # z = z.permute(0,2,1)\n",
    "            z = self.revin_layer(z, 'norm')\n",
    "            # z = z.permute(0,2,1)\n",
    "            \n",
    "        # do patching\n",
    "        if self.padding_patch == 'end':\n",
    "            z = self.padding_patch_layer(z)\n",
    "        # print(f'z.shape: {z.shape}')\n",
    "        z = z.unfold(dimension=-1, size=self.patch_len, step=self.stride)                   # z: [bs x nvars x patch_num x patch_len]\n",
    "        z = z.permute(0,1,3,2)                                                              # z: [bs x nvars x patch_len x patch_num]\n",
    "        \n",
    "        # model\n",
    "        z = self.backbone(z)                                                                # z: [bs x nvars x d_model x patch_num]\n",
    "        z = self.head(z)                                                                    # z: [bs x nvars x target_window] \n",
    "        \n",
    "        # denorm\n",
    "        if self.revin: \n",
    "            z = z.permute(0,2,1)\n",
    "            z = self.revin_layer(z, 'denorm')\n",
    "            # z = z.permute(0,2,1)\n",
    "        return z\n",
    "    \n",
    "    def create_pretrain_head(self, head_nf, vars, dropout):\n",
    "        return nn.Sequential(nn.Dropout(dropout),\n",
    "                    nn.Conv1d(head_nf, vars, 1)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear model\n",
    "class Linear(torch.nn.Module):\n",
    "    def __init__(self, c_in, context_window, target_window):\n",
    "        super().__init__()\n",
    "        self.c_in = c_in\n",
    "        self.context_winsoq = context_window\n",
    "        self.target_window = target_window\n",
    "\n",
    "        self.flatten = torch.nn.Flatten(start_dim=-2)\n",
    "\n",
    "        self.linear = torch.nn.Linear(c_in * context_window, target_window)\n",
    "    \n",
    "    def forward(self, x):                   # x: [bs x seq_len Ã— nvars]\n",
    "        x = self.flatten(x)                 # x: [bs x seq_len * nvars]\n",
    "        x = self.linear(x)                  # x: [bs x target_window]\n",
    "        return x\n",
    "    \n",
    "\n",
    "class moving_avg(torch.nn.Module):\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = torch.nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # padding on the both ends of time series\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class series_decomp(torch.nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "        return res, moving_mean\n",
    "\n",
    "class DLinear(torch.nn.Module):\n",
    "    def __init__(self, c_in, context_window, target_window):\n",
    "        super().__init__()\n",
    "        # Decompsition Kernel Size\n",
    "        kernel_size = 25\n",
    "        self.decompsition = series_decomp(kernel_size)\n",
    "        self.flatten_Seasonal = torch.nn.Flatten(start_dim=-2)\n",
    "        self.flatten_Trend = torch.nn.Flatten(start_dim=-2)\n",
    "        \n",
    "        self.Linear_Seasonal = torch.nn.Linear(c_in * context_window, target_window)\n",
    "        self.Linear_Trend = torch.nn.Linear(c_in * context_window, target_window)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [Batch, Input length, Channel]\n",
    "        seasonal_init, trend_init = self.decompsition(x)\n",
    "        seasonal_init = self.flatten_Seasonal(x)\n",
    "        trend_init = self.flatten_Trend(x)\n",
    "\n",
    "        seasonal_output = self.Linear_Seasonal(seasonal_init)\n",
    "        trend_output = self.Linear_Trend(trend_init)\n",
    "\n",
    "        x = seasonal_output + trend_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self, model, dataset, batch_size=128, lr=0.0001, epochs=100, target_window=96, d_model=16, adjust_lr=True, adjust_factor=0.001):\n",
    "        self.model = model.to(\"cuda\")\n",
    "        self.batch_size = batch_size\n",
    "        train_dataset = dataset(mode=\"train\")\n",
    "        valid_dataset = dataset(mode=\"val\")\n",
    "        test_dataset = dataset(mode=\"test\")\n",
    "        self.train_datalen = len(train_dataset)\n",
    "        self.valid_datalen = len(valid_dataset)\n",
    "        self.train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "        self.test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        self.lr=lr\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.loss = torch.nn.MSELoss()\n",
    "        self.epochs = epochs\n",
    "        self.target_window=target_window\n",
    "        self.best_weight = self.model.state_dict()\n",
    "        self.d_model=d_model\n",
    "        self.adjust_lr = adjust_lr\n",
    "        self.adjust_factor = adjust_factor\n",
    "    \n",
    "    def adjust_learning_rate(self, steps, warmup_step=300, printout=True):\n",
    "        if steps**(-0.5) < steps * (warmup_step**-1.5):\n",
    "            lr_adjust = (16**-0.5) * (steps**-0.5) * self.adjust_factor\n",
    "        else:\n",
    "            lr_adjust = (16**-0.5) * (steps * (warmup_step**-1.5)) * self.adjust_factor\n",
    "\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr_adjust\n",
    "        if printout: \n",
    "            print('Updating learning rate to {}'.format(lr_adjust))\n",
    "        return \n",
    "\n",
    "    def train(self):\n",
    "        best_valid_loss = np.inf\n",
    "        train_history = []\n",
    "        valid_history = []\n",
    "        train_steps = 1\n",
    "        if self.adjust_lr:\n",
    "            self.adjust_learning_rate(train_steps)\n",
    "        for epoch in range(self.epochs):\n",
    "            #train\n",
    "            self.model.train()\n",
    "            iter_count = 0\n",
    "            total_loss = 0\n",
    "\n",
    "            for train_x, train_y in self.train_dataloader:\n",
    "                train_x = train_x.to(\"cuda\")\n",
    "                train_y = train_y.to(\"cuda\")\n",
    "                # print(f'train_x.shape: {train_x.shape}')\n",
    "                pred_y = self.model(train_x)\n",
    "                pred_y = pred_y[:, -self.target_window:, -1:].squeeze(-1)\n",
    "                loss = self.loss(pred_y, train_y)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                iter_count += 1\n",
    "                train_steps += 1\n",
    "            if self.adjust_lr:\n",
    "                self.adjust_learning_rate(train_steps)\n",
    "\n",
    "            #valid\n",
    "            self.model.eval()\n",
    "            valid_iter_count = 0\n",
    "            valid_total_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for valid_x, valid_y in self.valid_dataloader:\n",
    "                    valid_x = valid_x.to(\"cuda\")\n",
    "                    valid_y = valid_y.to(\"cuda\")\n",
    "                    pred_y = self.model(valid_x)\n",
    "                    pred_y = pred_y[:, -self.target_window:, -1:].squeeze(-1)\n",
    "                    loss = self.loss(pred_y, valid_y)\n",
    "                    valid_total_loss += loss.item()\n",
    "                    valid_iter_count += 1\n",
    "            \n",
    "            total_loss /= iter_count\n",
    "            valid_total_loss /= valid_iter_count\n",
    "            print(\"epoch: {} MSE loss: {:.4f} MSE valid loss: {:.4f}\".format(epoch, total_loss, valid_total_loss))\n",
    "            if best_valid_loss >= valid_total_loss:\n",
    "                self.best_weight = self.model.state_dict()\n",
    "                best_valid_loss = valid_total_loss\n",
    "                print(\"Best score! Weights of the model are updated!\")\n",
    "            train_history.append(total_loss)\n",
    "            valid_history.append(valid_total_loss)\n",
    "        return train_history, valid_history\n",
    "\n",
    "    def test(self):\n",
    "        self.model.load_state_dict(self.best_weight)\n",
    "        self.model.eval()\n",
    "        iter_count = 0\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for test_x, test_y in self.test_dataloader:\n",
    "                test_x = test_x.to(\"cuda\")\n",
    "                test_y = test_y.to(\"cuda\")\n",
    "                pred_y = self.model(test_x)\n",
    "                loss = self.loss(pred_y, test_y)\n",
    "                total_loss += loss.item()\n",
    "                iter_count += 1\n",
    "        total_loss /= iter_count\n",
    "        print(\"MSE test loss: {:.4f}\".format(total_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner_2:\n",
    "    def __init__(self, model, dataset, batch_size=128, lr=0.0001, epochs=100, target_window=96, d_model=16, adjust_lr=True, adjust_factor=0.001):\n",
    "        self.model = model.to(\"cuda\")\n",
    "        self.batch_size = batch_size\n",
    "        train_dataset = dataset(mode=\"train\")\n",
    "        valid_dataset = dataset(mode=\"val\")\n",
    "        test_dataset = dataset(mode=\"test\")\n",
    "        self.train_datalen = len(train_dataset)\n",
    "        self.valid_datalen = len(valid_dataset)\n",
    "        self.train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "        self.test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        self.lr=lr\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.loss = torch.nn.MSELoss()\n",
    "        self.epochs = epochs\n",
    "        self.target_window=target_window\n",
    "        self.best_weight = self.model.state_dict()\n",
    "        self.d_model=d_model\n",
    "        self.adjust_lr = adjust_lr\n",
    "        self.adjust_factor = adjust_factor\n",
    "    \n",
    "    def adjust_learning_rate(self, steps, warmup_step=300, printout=True):\n",
    "        if steps**(-0.5) < steps * (warmup_step**-1.5):\n",
    "            lr_adjust = (16**-0.5) * (steps**-0.5) * self.adjust_factor\n",
    "        else:\n",
    "            lr_adjust = (16**-0.5) * (steps * (warmup_step**-1.5)) * self.adjust_factor\n",
    "\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr_adjust\n",
    "        if printout: \n",
    "            print('Updating learning rate to {}'.format(lr_adjust))\n",
    "        return \n",
    "\n",
    "    def train(self):\n",
    "        best_valid_loss = np.inf\n",
    "        train_history = []\n",
    "        valid_history = []\n",
    "        train_steps = 1\n",
    "        if self.adjust_lr:\n",
    "            self.adjust_learning_rate(train_steps)\n",
    "        for epoch in range(self.epochs):\n",
    "            #train\n",
    "            self.model.train()\n",
    "            iter_count = 0\n",
    "            total_loss = 0\n",
    "\n",
    "            for train_x, train_y in self.train_dataloader:\n",
    "                train_x = train_x.to(\"cuda\")\n",
    "                train_y = train_y.to(\"cuda\")\n",
    "                # print(f'train_x.shape: {train_x.shape}')\n",
    "                pred_y = self.model(train_x)\n",
    "                # pred_y = pred_y[:, -self.target_window:, -1:].squeeze(-1)\n",
    "                loss = self.loss(pred_y, train_y)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                iter_count += 1\n",
    "                train_steps += 1\n",
    "            if self.adjust_lr:\n",
    "                self.adjust_learning_rate(train_steps)\n",
    "\n",
    "            #valid\n",
    "            self.model.eval()\n",
    "            valid_iter_count = 0\n",
    "            valid_total_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for valid_x, valid_y in self.valid_dataloader:\n",
    "                    valid_x = valid_x.to(\"cuda\")\n",
    "                    valid_y = valid_y.to(\"cuda\")\n",
    "                    pred_y = self.model(valid_x)\n",
    "                    # pred_y = pred_y[:, -self.target_window:, -1:].squeeze(-1)\n",
    "                    loss = self.loss(pred_y, valid_y)\n",
    "                    valid_total_loss += loss.item()\n",
    "                    valid_iter_count += 1\n",
    "            \n",
    "            total_loss /= iter_count\n",
    "            valid_total_loss /= valid_iter_count\n",
    "            print(\"epoch: {} MSE loss: {:.4f} MSE valid loss: {:.4f}\".format(epoch, total_loss, valid_total_loss))\n",
    "            if best_valid_loss >= valid_total_loss:\n",
    "                self.best_weight = self.model.state_dict()\n",
    "                best_valid_loss = valid_total_loss\n",
    "                print(\"Best score! Weights of the model are updated!\")\n",
    "            train_history.append(total_loss)\n",
    "            valid_history.append(valid_total_loss)\n",
    "        return train_history, valid_history\n",
    "\n",
    "    def test(self):\n",
    "        self.model.load_state_dict(self.best_weight)\n",
    "        self.model.eval()\n",
    "        iter_count = 0\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for test_x, test_y in self.test_dataloader:\n",
    "                test_x = test_x.to(\"cuda\")\n",
    "                test_y = test_y.to(\"cuda\")\n",
    "                pred_y = self.model(test_x)\n",
    "                loss = self.loss(pred_y, test_y)\n",
    "                total_loss += loss.item()\n",
    "                iter_count += 1\n",
    "        total_loss /= iter_count\n",
    "        print(\"MSE test loss: {:.4f}\".format(total_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 4.811252243246882e-07\n",
      "Updating learning rate to 3.1754264805429416e-05\n",
      "epoch: 0 MSE loss: 1.2979 MSE valid loss: 0.8108\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 6.302740438653415e-05\n",
      "epoch: 1 MSE loss: 0.6081 MSE valid loss: 0.7448\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 9.430054396763888e-05\n",
      "epoch: 2 MSE loss: 0.3239 MSE valid loss: 0.6305\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 0.00012557368354874362\n",
      "epoch: 3 MSE loss: 0.2492 MSE valid loss: 0.4601\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 0.00013846219390542782\n",
      "epoch: 4 MSE loss: 0.2122 MSE valid loss: 0.3251\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 0.0001264304343560434\n",
      "epoch: 5 MSE loss: 0.1892 MSE valid loss: 0.2723\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 0.00011707322644771175\n",
      "epoch: 6 MSE loss: 0.1774 MSE valid loss: 0.2567\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 0.00010952698858458088\n",
      "epoch: 7 MSE loss: 0.1699 MSE valid loss: 0.2524\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 0.00010327404809650345\n",
      "epoch: 8 MSE loss: 0.1645 MSE valid loss: 0.2506\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 9.798272520870258e-05\n",
      "epoch: 9 MSE loss: 0.1601 MSE valid loss: 0.2534\n",
      "Updating learning rate to 9.342938659399198e-05\n",
      "epoch: 10 MSE loss: 0.1578 MSE valid loss: 0.2521\n",
      "Updating learning rate to 8.945703337056415e-05\n",
      "epoch: 11 MSE loss: 0.1543 MSE valid loss: 0.2572\n",
      "Updating learning rate to 8.595177052156611e-05\n",
      "epoch: 12 MSE loss: 0.1523 MSE valid loss: 0.2502\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 8.282869524032146e-05\n",
      "epoch: 13 MSE loss: 0.1506 MSE valid loss: 0.2512\n",
      "Updating learning rate to 8.002304995805999e-05\n",
      "epoch: 14 MSE loss: 0.1481 MSE valid loss: 0.2496\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 7.748446592171796e-05\n",
      "epoch: 15 MSE loss: 0.1466 MSE valid loss: 0.2517\n",
      "Updating learning rate to 7.517309741553296e-05\n",
      "epoch: 16 MSE loss: 0.1439 MSE valid loss: 0.2483\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 7.305695402335044e-05\n",
      "epoch: 17 MSE loss: 0.1438 MSE valid loss: 0.2508\n",
      "Updating learning rate to 7.111001549857179e-05\n",
      "epoch: 18 MSE loss: 0.1420 MSE valid loss: 0.2523\n",
      "Updating learning rate to 6.931087162517846e-05\n",
      "epoch: 19 MSE loss: 0.1408 MSE valid loss: 0.2497\n",
      "Updating learning rate to 6.76417225936176e-05\n",
      "epoch: 20 MSE loss: 0.1400 MSE valid loss: 0.2480\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 6.608763214317867e-05\n",
      "epoch: 21 MSE loss: 0.1386 MSE valid loss: 0.2515\n",
      "Updating learning rate to 6.46359612493774e-05\n",
      "epoch: 22 MSE loss: 0.1389 MSE valid loss: 0.2460\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 6.327593294406921e-05\n",
      "epoch: 23 MSE loss: 0.1371 MSE valid loss: 0.2456\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 6.199829383043036e-05\n",
      "epoch: 24 MSE loss: 0.1369 MSE valid loss: 0.2459\n",
      "Updating learning rate to 6.079504788572469e-05\n",
      "epoch: 25 MSE loss: 0.1358 MSE valid loss: 0.2463\n",
      "Updating learning rate to 5.965924498791846e-05\n",
      "epoch: 26 MSE loss: 0.1351 MSE valid loss: 0.2418\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 5.8584811349126495e-05\n",
      "epoch: 27 MSE loss: 0.1345 MSE valid loss: 0.2440\n",
      "Updating learning rate to 5.7566412382313e-05\n",
      "epoch: 28 MSE loss: 0.1335 MSE valid loss: 0.2409\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 5.659934091583233e-05\n",
      "epoch: 29 MSE loss: 0.1333 MSE valid loss: 0.2435\n",
      "Updating learning rate to 5.567942539842175e-05\n",
      "epoch: 30 MSE loss: 0.1326 MSE valid loss: 0.2405\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 5.4802954002676805e-05\n",
      "epoch: 31 MSE loss: 0.1320 MSE valid loss: 0.2423\n",
      "Updating learning rate to 5.39666114720432e-05\n",
      "epoch: 32 MSE loss: 0.1319 MSE valid loss: 0.2416\n",
      "Updating learning rate to 5.316742625738918e-05\n",
      "epoch: 33 MSE loss: 0.1312 MSE valid loss: 0.2423\n",
      "Updating learning rate to 5.240272601878982e-05\n",
      "epoch: 34 MSE loss: 0.1300 MSE valid loss: 0.2449\n",
      "Updating learning rate to 5.1670099971819504e-05\n",
      "epoch: 35 MSE loss: 0.1304 MSE valid loss: 0.2424\n",
      "Updating learning rate to 5.096736686795811e-05\n",
      "epoch: 36 MSE loss: 0.1300 MSE valid loss: 0.2365\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 5.0292547639160534e-05\n",
      "epoch: 37 MSE loss: 0.1293 MSE valid loss: 0.2407\n",
      "Updating learning rate to 4.96438419243461e-05\n",
      "epoch: 38 MSE loss: 0.1291 MSE valid loss: 0.2420\n",
      "Updating learning rate to 4.901960784313725e-05\n",
      "epoch: 39 MSE loss: 0.1278 MSE valid loss: 0.2389\n",
      "Updating learning rate to 4.841834449897069e-05\n",
      "epoch: 40 MSE loss: 0.1286 MSE valid loss: 0.2390\n",
      "Updating learning rate to 4.783867678672252e-05\n",
      "epoch: 41 MSE loss: 0.1275 MSE valid loss: 0.2370\n",
      "Updating learning rate to 4.727934215451461e-05\n",
      "epoch: 42 MSE loss: 0.1274 MSE valid loss: 0.2413\n",
      "Updating learning rate to 4.6739179029417445e-05\n",
      "epoch: 43 MSE loss: 0.1272 MSE valid loss: 0.2453\n",
      "Updating learning rate to 4.621711666540848e-05\n",
      "epoch: 44 MSE loss: 0.1264 MSE valid loss: 0.2392\n",
      "Updating learning rate to 4.571216621155238e-05\n",
      "epoch: 45 MSE loss: 0.1268 MSE valid loss: 0.2392\n",
      "Updating learning rate to 4.522341283077635e-05\n",
      "epoch: 46 MSE loss: 0.1260 MSE valid loss: 0.2419\n",
      "Updating learning rate to 4.4750008726252546e-05\n",
      "epoch: 47 MSE loss: 0.1264 MSE valid loss: 0.2382\n",
      "Updating learning rate to 4.4291166954394496e-05\n",
      "epoch: 48 MSE loss: 0.1254 MSE valid loss: 0.2403\n",
      "Updating learning rate to 4.384615592171157e-05\n",
      "epoch: 49 MSE loss: 0.1255 MSE valid loss: 0.2378\n",
      "Updating learning rate to 4.341429447794924e-05\n",
      "epoch: 50 MSE loss: 0.1255 MSE valid loss: 0.2392\n",
      "Updating learning rate to 4.299494753063186e-05\n",
      "epoch: 51 MSE loss: 0.1244 MSE valid loss: 0.2369\n",
      "Updating learning rate to 4.2587522116770666e-05\n",
      "epoch: 52 MSE loss: 0.1237 MSE valid loss: 0.2379\n",
      "Updating learning rate to 4.219146387646126e-05\n",
      "epoch: 53 MSE loss: 0.1237 MSE valid loss: 0.2386\n",
      "Updating learning rate to 4.1806253880665696e-05\n",
      "epoch: 54 MSE loss: 0.1231 MSE valid loss: 0.2351\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 4.143140577189121e-05\n",
      "epoch: 55 MSE loss: 0.1233 MSE valid loss: 0.2415\n",
      "Updating learning rate to 4.106646318193315e-05\n",
      "epoch: 56 MSE loss: 0.1231 MSE valid loss: 0.2397\n",
      "Updating learning rate to 4.071099739550263e-05\n",
      "epoch: 57 MSE loss: 0.1226 MSE valid loss: 0.2391\n",
      "Updating learning rate to 4.0364605232539164e-05\n",
      "epoch: 58 MSE loss: 0.1230 MSE valid loss: 0.2382\n",
      "Updating learning rate to 4.0026907125422175e-05\n",
      "epoch: 59 MSE loss: 0.1226 MSE valid loss: 0.2444\n",
      "Updating learning rate to 3.969754537023144e-05\n",
      "epoch: 60 MSE loss: 0.1222 MSE valid loss: 0.2408\n",
      "Updating learning rate to 3.937618253373847e-05\n",
      "epoch: 61 MSE loss: 0.1222 MSE valid loss: 0.2418\n",
      "Updating learning rate to 3.90625e-05\n",
      "epoch: 62 MSE loss: 0.1217 MSE valid loss: 0.2396\n",
      "Updating learning rate to 3.875619664232189e-05\n",
      "epoch: 63 MSE loss: 0.1214 MSE valid loss: 0.2409\n",
      "Updating learning rate to 3.845698760800996e-05\n",
      "epoch: 64 MSE loss: 0.1214 MSE valid loss: 0.2362\n",
      "Updating learning rate to 3.816460320475954e-05\n",
      "epoch: 65 MSE loss: 0.1211 MSE valid loss: 0.2382\n",
      "Updating learning rate to 3.787878787878788e-05\n",
      "epoch: 66 MSE loss: 0.1206 MSE valid loss: 0.2397\n",
      "Updating learning rate to 3.759929927590882e-05\n",
      "epoch: 67 MSE loss: 0.1203 MSE valid loss: 0.2380\n",
      "Updating learning rate to 3.732590737770921e-05\n",
      "epoch: 68 MSE loss: 0.1204 MSE valid loss: 0.2403\n",
      "Updating learning rate to 3.7058393705829335e-05\n",
      "epoch: 69 MSE loss: 0.1202 MSE valid loss: 0.2392\n",
      "Updating learning rate to 3.679655058809149e-05\n",
      "epoch: 70 MSE loss: 0.1199 MSE valid loss: 0.2414\n",
      "Updating learning rate to 3.654018048087443e-05\n",
      "epoch: 71 MSE loss: 0.1200 MSE valid loss: 0.2401\n",
      "Updating learning rate to 3.6289095342709304e-05\n",
      "epoch: 72 MSE loss: 0.1196 MSE valid loss: 0.2401\n",
      "Updating learning rate to 3.604311605458291e-05\n",
      "epoch: 73 MSE loss: 0.1191 MSE valid loss: 0.2436\n",
      "Updating learning rate to 3.58020718828878e-05\n",
      "epoch: 74 MSE loss: 0.1197 MSE valid loss: 0.2414\n",
      "Updating learning rate to 3.5565799981359994e-05\n",
      "epoch: 75 MSE loss: 0.1191 MSE valid loss: 0.2422\n",
      "Updating learning rate to 3.5334144928703015e-05\n",
      "epoch: 76 MSE loss: 0.1189 MSE valid loss: 0.2407\n",
      "Updating learning rate to 3.510695829891526e-05\n",
      "epoch: 77 MSE loss: 0.1197 MSE valid loss: 0.2444\n",
      "Updating learning rate to 3.488409826162172e-05\n",
      "epoch: 78 MSE loss: 0.1183 MSE valid loss: 0.2376\n",
      "Updating learning rate to 3.4665429209964995e-05\n",
      "epoch: 79 MSE loss: 0.1184 MSE valid loss: 0.2395\n",
      "Updating learning rate to 3.445082141383733e-05\n",
      "epoch: 80 MSE loss: 0.1181 MSE valid loss: 0.2472\n",
      "Updating learning rate to 3.424015069643934e-05\n",
      "epoch: 81 MSE loss: 0.1177 MSE valid loss: 0.2422\n",
      "Updating learning rate to 3.4033298132333136e-05\n",
      "epoch: 82 MSE loss: 0.1178 MSE valid loss: 0.2414\n",
      "Updating learning rate to 3.383014976532195e-05\n",
      "epoch: 83 MSE loss: 0.1177 MSE valid loss: 0.2402\n",
      "Updating learning rate to 3.3630596344635906e-05\n",
      "epoch: 84 MSE loss: 0.1172 MSE valid loss: 0.2425\n",
      "Updating learning rate to 3.343453307803635e-05\n",
      "epoch: 85 MSE loss: 0.1180 MSE valid loss: 0.2428\n",
      "Updating learning rate to 3.3241859400571366e-05\n",
      "epoch: 86 MSE loss: 0.1178 MSE valid loss: 0.2453\n",
      "Updating learning rate to 3.305247875782325e-05\n",
      "epoch: 87 MSE loss: 0.1173 MSE valid loss: 0.2455\n",
      "Updating learning rate to 3.286629840258662e-05\n",
      "epoch: 88 MSE loss: 0.1174 MSE valid loss: 0.2422\n",
      "Updating learning rate to 3.268322920400467e-05\n",
      "epoch: 89 MSE loss: 0.1170 MSE valid loss: 0.2431\n",
      "Updating learning rate to 3.250318546827149e-05\n",
      "epoch: 90 MSE loss: 0.1163 MSE valid loss: 0.2446\n",
      "Updating learning rate to 3.232608477008077e-05\n",
      "epoch: 91 MSE loss: 0.1165 MSE valid loss: 0.2445\n",
      "Updating learning rate to 3.21518477940684e-05\n",
      "epoch: 92 MSE loss: 0.1168 MSE valid loss: 0.2461\n",
      "Updating learning rate to 3.198039818555568e-05\n",
      "epoch: 93 MSE loss: 0.1166 MSE valid loss: 0.2467\n",
      "Updating learning rate to 3.181166240995547e-05\n",
      "epoch: 94 MSE loss: 0.1166 MSE valid loss: 0.2464\n",
      "Updating learning rate to 3.1645569620253167e-05\n",
      "epoch: 95 MSE loss: 0.1159 MSE valid loss: 0.2417\n",
      "Updating learning rate to 3.148205153202001e-05\n",
      "epoch: 96 MSE loss: 0.1166 MSE valid loss: 0.2440\n",
      "Updating learning rate to 3.1321042305458135e-05\n",
      "epoch: 97 MSE loss: 0.1163 MSE valid loss: 0.2451\n",
      "Updating learning rate to 3.1162478434014266e-05\n",
      "epoch: 98 MSE loss: 0.1159 MSE valid loss: 0.2441\n",
      "Updating learning rate to 3.100629863913435e-05\n",
      "epoch: 99 MSE loss: 0.1161 MSE valid loss: 0.2534\n",
      "Updating learning rate to 4.811252243246882e-07\n",
      "Updating learning rate to 3.1754264805429416e-05\n",
      "epoch: 0 MSE loss: 1.5605 MSE valid loss: 1.2632\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 6.302740438653415e-05\n",
      "epoch: 1 MSE loss: 0.6145 MSE valid loss: 0.8255\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 9.430054396763888e-05\n",
      "epoch: 2 MSE loss: 0.3369 MSE valid loss: 0.5786\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 0.00012557368354874362\n",
      "epoch: 3 MSE loss: 0.2625 MSE valid loss: 0.3877\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 0.00013846219390542782\n",
      "epoch: 4 MSE loss: 0.2207 MSE valid loss: 0.3230\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 0.0001264304343560434\n",
      "epoch: 5 MSE loss: 0.1969 MSE valid loss: 0.3021\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 0.00011707322644771175\n",
      "epoch: 6 MSE loss: 0.1838 MSE valid loss: 0.2959\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 0.00010952698858458088\n",
      "epoch: 7 MSE loss: 0.1756 MSE valid loss: 0.2873\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 0.00010327404809650345\n",
      "epoch: 8 MSE loss: 0.1686 MSE valid loss: 0.2851\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 9.798272520870258e-05\n",
      "epoch: 9 MSE loss: 0.1639 MSE valid loss: 0.2817\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 9.342938659399198e-05\n",
      "epoch: 10 MSE loss: 0.1593 MSE valid loss: 0.2764\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 8.945703337056415e-05\n",
      "epoch: 11 MSE loss: 0.1560 MSE valid loss: 0.2780\n",
      "Updating learning rate to 8.595177052156611e-05\n",
      "epoch: 12 MSE loss: 0.1534 MSE valid loss: 0.2765\n",
      "Updating learning rate to 8.282869524032146e-05\n",
      "epoch: 13 MSE loss: 0.1513 MSE valid loss: 0.2748\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 8.002304995805999e-05\n",
      "epoch: 14 MSE loss: 0.1486 MSE valid loss: 0.2698\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 7.748446592171796e-05\n",
      "epoch: 15 MSE loss: 0.1475 MSE valid loss: 0.2691\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 7.517309741553296e-05\n",
      "epoch: 16 MSE loss: 0.1464 MSE valid loss: 0.2714\n",
      "Updating learning rate to 7.305695402335044e-05\n",
      "epoch: 17 MSE loss: 0.1445 MSE valid loss: 0.2605\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 7.111001549857179e-05\n",
      "epoch: 18 MSE loss: 0.1432 MSE valid loss: 0.2682\n",
      "Updating learning rate to 6.931087162517846e-05\n",
      "epoch: 19 MSE loss: 0.1421 MSE valid loss: 0.2664\n",
      "Updating learning rate to 6.76417225936176e-05\n",
      "epoch: 20 MSE loss: 0.1406 MSE valid loss: 0.2642\n",
      "Updating learning rate to 6.608763214317867e-05\n",
      "epoch: 21 MSE loss: 0.1398 MSE valid loss: 0.2632\n",
      "Updating learning rate to 6.46359612493774e-05\n",
      "epoch: 22 MSE loss: 0.1387 MSE valid loss: 0.2613\n",
      "Updating learning rate to 6.327593294406921e-05\n",
      "epoch: 23 MSE loss: 0.1377 MSE valid loss: 0.2717\n",
      "Updating learning rate to 6.199829383043036e-05\n",
      "epoch: 24 MSE loss: 0.1362 MSE valid loss: 0.2617\n",
      "Updating learning rate to 6.079504788572469e-05\n",
      "epoch: 25 MSE loss: 0.1358 MSE valid loss: 0.2625\n",
      "Updating learning rate to 5.965924498791846e-05\n",
      "epoch: 26 MSE loss: 0.1354 MSE valid loss: 0.2596\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 5.8584811349126495e-05\n",
      "epoch: 27 MSE loss: 0.1344 MSE valid loss: 0.2619\n",
      "Updating learning rate to 5.7566412382313e-05\n",
      "epoch: 28 MSE loss: 0.1331 MSE valid loss: 0.2610\n",
      "Updating learning rate to 5.659934091583233e-05\n",
      "epoch: 29 MSE loss: 0.1331 MSE valid loss: 0.2612\n",
      "Updating learning rate to 5.567942539842175e-05\n",
      "epoch: 30 MSE loss: 0.1322 MSE valid loss: 0.2681\n",
      "Updating learning rate to 5.4802954002676805e-05\n",
      "epoch: 31 MSE loss: 0.1312 MSE valid loss: 0.2688\n",
      "Updating learning rate to 5.39666114720432e-05\n",
      "epoch: 32 MSE loss: 0.1305 MSE valid loss: 0.2640\n",
      "Updating learning rate to 5.316742625738918e-05\n",
      "epoch: 33 MSE loss: 0.1304 MSE valid loss: 0.2574\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 5.240272601878982e-05\n",
      "epoch: 34 MSE loss: 0.1295 MSE valid loss: 0.2633\n",
      "Updating learning rate to 5.1670099971819504e-05\n",
      "epoch: 35 MSE loss: 0.1292 MSE valid loss: 0.2596\n",
      "Updating learning rate to 5.096736686795811e-05\n",
      "epoch: 36 MSE loss: 0.1288 MSE valid loss: 0.2540\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 5.0292547639160534e-05\n",
      "epoch: 37 MSE loss: 0.1277 MSE valid loss: 0.2618\n",
      "Updating learning rate to 4.96438419243461e-05\n",
      "epoch: 38 MSE loss: 0.1276 MSE valid loss: 0.2627\n",
      "Updating learning rate to 4.901960784313725e-05\n",
      "epoch: 39 MSE loss: 0.1268 MSE valid loss: 0.2592\n",
      "Updating learning rate to 4.841834449897069e-05\n",
      "epoch: 40 MSE loss: 0.1263 MSE valid loss: 0.2569\n",
      "Updating learning rate to 4.783867678672252e-05\n",
      "epoch: 41 MSE loss: 0.1262 MSE valid loss: 0.2635\n",
      "Updating learning rate to 4.727934215451461e-05\n",
      "epoch: 42 MSE loss: 0.1259 MSE valid loss: 0.2625\n",
      "Updating learning rate to 4.6739179029417445e-05\n",
      "epoch: 43 MSE loss: 0.1259 MSE valid loss: 0.2625\n",
      "Updating learning rate to 4.621711666540848e-05\n",
      "epoch: 44 MSE loss: 0.1249 MSE valid loss: 0.2577\n",
      "Updating learning rate to 4.571216621155238e-05\n",
      "epoch: 45 MSE loss: 0.1251 MSE valid loss: 0.2594\n",
      "Updating learning rate to 4.522341283077635e-05\n",
      "epoch: 46 MSE loss: 0.1247 MSE valid loss: 0.2663\n",
      "Updating learning rate to 4.4750008726252546e-05\n",
      "epoch: 47 MSE loss: 0.1237 MSE valid loss: 0.2634\n",
      "Updating learning rate to 4.4291166954394496e-05\n",
      "epoch: 48 MSE loss: 0.1237 MSE valid loss: 0.2590\n",
      "Updating learning rate to 4.384615592171157e-05\n",
      "epoch: 49 MSE loss: 0.1232 MSE valid loss: 0.2606\n",
      "Updating learning rate to 4.341429447794924e-05\n",
      "epoch: 50 MSE loss: 0.1230 MSE valid loss: 0.2608\n",
      "Updating learning rate to 4.299494753063186e-05\n",
      "epoch: 51 MSE loss: 0.1226 MSE valid loss: 0.2582\n",
      "Updating learning rate to 4.2587522116770666e-05\n",
      "epoch: 52 MSE loss: 0.1222 MSE valid loss: 0.2608\n",
      "Updating learning rate to 4.219146387646126e-05\n",
      "epoch: 53 MSE loss: 0.1223 MSE valid loss: 0.2656\n",
      "Updating learning rate to 4.1806253880665696e-05\n",
      "epoch: 54 MSE loss: 0.1214 MSE valid loss: 0.2616\n",
      "Updating learning rate to 4.143140577189121e-05\n",
      "epoch: 55 MSE loss: 0.1214 MSE valid loss: 0.2643\n",
      "Updating learning rate to 4.106646318193315e-05\n",
      "epoch: 56 MSE loss: 0.1219 MSE valid loss: 0.2618\n",
      "Updating learning rate to 4.071099739550263e-05\n",
      "epoch: 57 MSE loss: 0.1200 MSE valid loss: 0.2603\n",
      "Updating learning rate to 4.0364605232539164e-05\n",
      "epoch: 58 MSE loss: 0.1211 MSE valid loss: 0.2632\n",
      "Updating learning rate to 4.0026907125422175e-05\n",
      "epoch: 59 MSE loss: 0.1198 MSE valid loss: 0.2700\n",
      "Updating learning rate to 3.969754537023144e-05\n",
      "epoch: 60 MSE loss: 0.1197 MSE valid loss: 0.2656\n",
      "Updating learning rate to 3.937618253373847e-05\n",
      "epoch: 61 MSE loss: 0.1195 MSE valid loss: 0.2694\n",
      "Updating learning rate to 3.90625e-05\n",
      "epoch: 62 MSE loss: 0.1202 MSE valid loss: 0.2635\n",
      "Updating learning rate to 3.875619664232189e-05\n",
      "epoch: 63 MSE loss: 0.1193 MSE valid loss: 0.2695\n",
      "Updating learning rate to 3.845698760800996e-05\n",
      "epoch: 64 MSE loss: 0.1189 MSE valid loss: 0.2665\n",
      "Updating learning rate to 3.816460320475954e-05\n",
      "epoch: 65 MSE loss: 0.1184 MSE valid loss: 0.2652\n",
      "Updating learning rate to 3.787878787878788e-05\n",
      "epoch: 66 MSE loss: 0.1184 MSE valid loss: 0.2676\n",
      "Updating learning rate to 3.759929927590882e-05\n",
      "epoch: 67 MSE loss: 0.1178 MSE valid loss: 0.2613\n",
      "Updating learning rate to 3.732590737770921e-05\n",
      "epoch: 68 MSE loss: 0.1182 MSE valid loss: 0.2654\n",
      "Updating learning rate to 3.7058393705829335e-05\n",
      "epoch: 69 MSE loss: 0.1189 MSE valid loss: 0.2707\n",
      "Updating learning rate to 3.679655058809149e-05\n",
      "epoch: 70 MSE loss: 0.1173 MSE valid loss: 0.2621\n",
      "Updating learning rate to 3.654018048087443e-05\n",
      "epoch: 71 MSE loss: 0.1169 MSE valid loss: 0.2706\n",
      "Updating learning rate to 3.6289095342709304e-05\n",
      "epoch: 72 MSE loss: 0.1168 MSE valid loss: 0.2684\n",
      "Updating learning rate to 3.604311605458291e-05\n",
      "epoch: 73 MSE loss: 0.1172 MSE valid loss: 0.2744\n",
      "Updating learning rate to 3.58020718828878e-05\n",
      "epoch: 74 MSE loss: 0.1169 MSE valid loss: 0.2675\n",
      "Updating learning rate to 3.5565799981359994e-05\n",
      "epoch: 75 MSE loss: 0.1166 MSE valid loss: 0.2751\n",
      "Updating learning rate to 3.5334144928703015e-05\n",
      "epoch: 76 MSE loss: 0.1162 MSE valid loss: 0.2672\n",
      "Updating learning rate to 3.510695829891526e-05\n",
      "epoch: 77 MSE loss: 0.1164 MSE valid loss: 0.2728\n",
      "Updating learning rate to 3.488409826162172e-05\n",
      "epoch: 78 MSE loss: 0.1163 MSE valid loss: 0.2686\n",
      "Updating learning rate to 3.4665429209964995e-05\n",
      "epoch: 79 MSE loss: 0.1161 MSE valid loss: 0.2809\n",
      "Updating learning rate to 3.445082141383733e-05\n",
      "epoch: 80 MSE loss: 0.1154 MSE valid loss: 0.2702\n",
      "Updating learning rate to 3.424015069643934e-05\n",
      "epoch: 81 MSE loss: 0.1153 MSE valid loss: 0.2718\n",
      "Updating learning rate to 3.4033298132333136e-05\n",
      "epoch: 82 MSE loss: 0.1150 MSE valid loss: 0.2717\n",
      "Updating learning rate to 3.383014976532195e-05\n",
      "epoch: 83 MSE loss: 0.1151 MSE valid loss: 0.2740\n",
      "Updating learning rate to 3.3630596344635906e-05\n",
      "epoch: 84 MSE loss: 0.1147 MSE valid loss: 0.2750\n",
      "Updating learning rate to 3.343453307803635e-05\n",
      "epoch: 85 MSE loss: 0.1152 MSE valid loss: 0.2763\n",
      "Updating learning rate to 3.3241859400571366e-05\n",
      "epoch: 86 MSE loss: 0.1143 MSE valid loss: 0.2728\n",
      "Updating learning rate to 3.305247875782325e-05\n",
      "epoch: 87 MSE loss: 0.1146 MSE valid loss: 0.2795\n",
      "Updating learning rate to 3.286629840258662e-05\n",
      "epoch: 88 MSE loss: 0.1142 MSE valid loss: 0.2775\n",
      "Updating learning rate to 3.268322920400467e-05\n",
      "epoch: 89 MSE loss: 0.1139 MSE valid loss: 0.2756\n",
      "Updating learning rate to 3.250318546827149e-05\n",
      "epoch: 90 MSE loss: 0.1142 MSE valid loss: 0.2760\n",
      "Updating learning rate to 3.232608477008077e-05\n",
      "epoch: 91 MSE loss: 0.1144 MSE valid loss: 0.2845\n",
      "Updating learning rate to 3.21518477940684e-05\n",
      "epoch: 92 MSE loss: 0.1146 MSE valid loss: 0.2803\n",
      "Updating learning rate to 3.198039818555568e-05\n",
      "epoch: 93 MSE loss: 0.1135 MSE valid loss: 0.2765\n",
      "Updating learning rate to 3.181166240995547e-05\n",
      "epoch: 94 MSE loss: 0.1141 MSE valid loss: 0.2815\n",
      "Updating learning rate to 3.1645569620253167e-05\n",
      "epoch: 95 MSE loss: 0.1139 MSE valid loss: 0.2850\n",
      "Updating learning rate to 3.148205153202001e-05\n",
      "epoch: 96 MSE loss: 0.1132 MSE valid loss: 0.2785\n",
      "Updating learning rate to 3.1321042305458135e-05\n",
      "epoch: 97 MSE loss: 0.1129 MSE valid loss: 0.2814\n",
      "Updating learning rate to 3.1162478434014266e-05\n",
      "epoch: 98 MSE loss: 0.1128 MSE valid loss: 0.2782\n",
      "Updating learning rate to 3.100629863913435e-05\n",
      "epoch: 99 MSE loss: 0.1131 MSE valid loss: 0.2802\n",
      "Updating learning rate to 4.811252243246882e-08\n",
      "Updating learning rate to 3.175426480542942e-06\n",
      "epoch: 0 MSE loss: 0.2377 MSE valid loss: 0.1347\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 6.302740438653415e-06\n",
      "epoch: 1 MSE loss: 0.2281 MSE valid loss: 0.1256\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 9.430054396763887e-06\n",
      "epoch: 2 MSE loss: 0.2113 MSE valid loss: 0.1156\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 1.2557368354874362e-05\n",
      "epoch: 3 MSE loss: 0.1952 MSE valid loss: 0.1083\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 1.3846219390542782e-05\n",
      "epoch: 4 MSE loss: 0.1836 MSE valid loss: 0.1045\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 1.264304343560434e-05\n",
      "epoch: 5 MSE loss: 0.1769 MSE valid loss: 0.1018\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 1.1707322644771175e-05\n",
      "epoch: 6 MSE loss: 0.1730 MSE valid loss: 0.1003\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 1.0952698858458087e-05\n",
      "epoch: 7 MSE loss: 0.1700 MSE valid loss: 0.0992\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 1.0327404809650346e-05\n",
      "epoch: 8 MSE loss: 0.1678 MSE valid loss: 0.0984\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 9.798272520870257e-06\n",
      "epoch: 9 MSE loss: 0.1653 MSE valid loss: 0.0974\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 9.342938659399199e-06\n",
      "epoch: 10 MSE loss: 0.1643 MSE valid loss: 0.0969\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 8.945703337056415e-06\n",
      "epoch: 11 MSE loss: 0.1615 MSE valid loss: 0.0962\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 8.595177052156612e-06\n",
      "epoch: 12 MSE loss: 0.1609 MSE valid loss: 0.0959\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 8.282869524032146e-06\n",
      "epoch: 13 MSE loss: 0.1604 MSE valid loss: 0.0955\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 8.002304995806e-06\n",
      "epoch: 14 MSE loss: 0.1585 MSE valid loss: 0.0953\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 7.748446592171797e-06\n",
      "epoch: 15 MSE loss: 0.1575 MSE valid loss: 0.0950\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 7.517309741553296e-06\n",
      "epoch: 16 MSE loss: 0.1566 MSE valid loss: 0.0950\n",
      "Updating learning rate to 7.305695402335044e-06\n",
      "epoch: 17 MSE loss: 0.1564 MSE valid loss: 0.0947\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 7.1110015498571785e-06\n",
      "epoch: 18 MSE loss: 0.1547 MSE valid loss: 0.0946\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 6.931087162517846e-06\n",
      "epoch: 19 MSE loss: 0.1538 MSE valid loss: 0.0945\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 6.76417225936176e-06\n",
      "epoch: 20 MSE loss: 0.1538 MSE valid loss: 0.0941\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 6.608763214317868e-06\n",
      "epoch: 21 MSE loss: 0.1529 MSE valid loss: 0.0943\n",
      "Updating learning rate to 6.463596124937739e-06\n",
      "epoch: 22 MSE loss: 0.1530 MSE valid loss: 0.0940\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 6.327593294406922e-06\n",
      "epoch: 23 MSE loss: 0.1523 MSE valid loss: 0.0940\n",
      "Updating learning rate to 6.199829383043036e-06\n",
      "epoch: 24 MSE loss: 0.1519 MSE valid loss: 0.0941\n",
      "Updating learning rate to 6.079504788572469e-06\n",
      "epoch: 25 MSE loss: 0.1506 MSE valid loss: 0.0939\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 5.965924498791846e-06\n",
      "epoch: 26 MSE loss: 0.1501 MSE valid loss: 0.0938\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 5.8584811349126495e-06\n",
      "epoch: 27 MSE loss: 0.1493 MSE valid loss: 0.0938\n",
      "Best score! Weights of the model are updated!\n",
      "Updating learning rate to 5.7566412382313e-06\n",
      "epoch: 28 MSE loss: 0.1501 MSE valid loss: 0.0939\n",
      "Updating learning rate to 5.659934091583234e-06\n",
      "epoch: 29 MSE loss: 0.1486 MSE valid loss: 0.0940\n",
      "Updating learning rate to 5.567942539842175e-06\n",
      "epoch: 30 MSE loss: 0.1485 MSE valid loss: 0.0939\n",
      "Updating learning rate to 5.48029540026768e-06\n",
      "epoch: 31 MSE loss: 0.1477 MSE valid loss: 0.0940\n",
      "Updating learning rate to 5.39666114720432e-06\n",
      "epoch: 32 MSE loss: 0.1472 MSE valid loss: 0.0940\n",
      "Updating learning rate to 5.316742625738919e-06\n",
      "epoch: 33 MSE loss: 0.1476 MSE valid loss: 0.0941\n",
      "Updating learning rate to 5.240272601878983e-06\n",
      "epoch: 34 MSE loss: 0.1472 MSE valid loss: 0.0942\n",
      "Updating learning rate to 5.16700999718195e-06\n",
      "epoch: 35 MSE loss: 0.1462 MSE valid loss: 0.0944\n",
      "Updating learning rate to 5.096736686795811e-06\n",
      "epoch: 36 MSE loss: 0.1461 MSE valid loss: 0.0946\n",
      "Updating learning rate to 5.029254763916053e-06\n",
      "epoch: 37 MSE loss: 0.1461 MSE valid loss: 0.0946\n",
      "Updating learning rate to 4.964384192434611e-06\n",
      "epoch: 38 MSE loss: 0.1469 MSE valid loss: 0.0946\n",
      "Updating learning rate to 4.901960784313726e-06\n",
      "epoch: 39 MSE loss: 0.1444 MSE valid loss: 0.0947\n",
      "Updating learning rate to 4.841834449897069e-06\n",
      "epoch: 40 MSE loss: 0.1451 MSE valid loss: 0.0950\n",
      "Updating learning rate to 4.783867678672252e-06\n",
      "epoch: 41 MSE loss: 0.1440 MSE valid loss: 0.0951\n",
      "Updating learning rate to 4.727934215451461e-06\n",
      "epoch: 42 MSE loss: 0.1440 MSE valid loss: 0.0953\n",
      "Updating learning rate to 4.673917902941745e-06\n",
      "epoch: 43 MSE loss: 0.1441 MSE valid loss: 0.0954\n",
      "Updating learning rate to 4.621711666540848e-06\n",
      "epoch: 44 MSE loss: 0.1430 MSE valid loss: 0.0955\n",
      "Updating learning rate to 4.571216621155238e-06\n",
      "epoch: 45 MSE loss: 0.1424 MSE valid loss: 0.0958\n",
      "Updating learning rate to 4.5223412830776355e-06\n",
      "epoch: 46 MSE loss: 0.1432 MSE valid loss: 0.0959\n",
      "Updating learning rate to 4.475000872625255e-06\n",
      "epoch: 47 MSE loss: 0.1420 MSE valid loss: 0.0960\n",
      "Updating learning rate to 4.42911669543945e-06\n",
      "epoch: 48 MSE loss: 0.1418 MSE valid loss: 0.0963\n",
      "Updating learning rate to 4.3846155921711576e-06\n",
      "epoch: 49 MSE loss: 0.1413 MSE valid loss: 0.0967\n",
      "Updating learning rate to 4.341429447794925e-06\n",
      "epoch: 50 MSE loss: 0.1413 MSE valid loss: 0.0966\n",
      "Updating learning rate to 4.299494753063186e-06\n",
      "epoch: 51 MSE loss: 0.1404 MSE valid loss: 0.0969\n",
      "Updating learning rate to 4.258752211677067e-06\n",
      "epoch: 52 MSE loss: 0.1404 MSE valid loss: 0.0971\n",
      "Updating learning rate to 4.219146387646126e-06\n",
      "epoch: 53 MSE loss: 0.1407 MSE valid loss: 0.0975\n",
      "Updating learning rate to 4.18062538806657e-06\n",
      "epoch: 54 MSE loss: 0.1394 MSE valid loss: 0.0975\n",
      "Updating learning rate to 4.143140577189122e-06\n",
      "epoch: 55 MSE loss: 0.1392 MSE valid loss: 0.0978\n",
      "Updating learning rate to 4.106646318193315e-06\n",
      "epoch: 56 MSE loss: 0.1389 MSE valid loss: 0.0980\n",
      "Updating learning rate to 4.071099739550263e-06\n",
      "epoch: 57 MSE loss: 0.1388 MSE valid loss: 0.0983\n",
      "Updating learning rate to 4.036460523253916e-06\n",
      "epoch: 58 MSE loss: 0.1385 MSE valid loss: 0.0985\n",
      "Updating learning rate to 4.002690712542218e-06\n",
      "epoch: 59 MSE loss: 0.1396 MSE valid loss: 0.0985\n",
      "Updating learning rate to 3.969754537023144e-06\n",
      "epoch: 60 MSE loss: 0.1385 MSE valid loss: 0.0988\n",
      "Updating learning rate to 3.937618253373847e-06\n",
      "epoch: 61 MSE loss: 0.1386 MSE valid loss: 0.0990\n",
      "Updating learning rate to 3.90625e-06\n",
      "epoch: 62 MSE loss: 0.1374 MSE valid loss: 0.0992\n",
      "Updating learning rate to 3.8756196642321895e-06\n",
      "epoch: 63 MSE loss: 0.1382 MSE valid loss: 0.0993\n",
      "Updating learning rate to 3.845698760800996e-06\n",
      "epoch: 64 MSE loss: 0.1370 MSE valid loss: 0.0994\n",
      "Updating learning rate to 3.816460320475954e-06\n",
      "epoch: 65 MSE loss: 0.1370 MSE valid loss: 0.0998\n",
      "Updating learning rate to 3.7878787878787882e-06\n",
      "epoch: 66 MSE loss: 0.1375 MSE valid loss: 0.1002\n",
      "Updating learning rate to 3.759929927590882e-06\n",
      "epoch: 67 MSE loss: 0.1363 MSE valid loss: 0.1002\n",
      "Updating learning rate to 3.732590737770921e-06\n",
      "epoch: 68 MSE loss: 0.1363 MSE valid loss: 0.0999\n",
      "Updating learning rate to 3.7058393705829336e-06\n",
      "epoch: 69 MSE loss: 0.1364 MSE valid loss: 0.1001\n",
      "Updating learning rate to 3.6796550588091485e-06\n",
      "epoch: 70 MSE loss: 0.1351 MSE valid loss: 0.1003\n",
      "Updating learning rate to 3.6540180480874438e-06\n",
      "epoch: 71 MSE loss: 0.1358 MSE valid loss: 0.1005\n",
      "Updating learning rate to 3.6289095342709304e-06\n",
      "epoch: 72 MSE loss: 0.1357 MSE valid loss: 0.1006\n",
      "Updating learning rate to 3.604311605458291e-06\n",
      "epoch: 73 MSE loss: 0.1352 MSE valid loss: 0.1008\n",
      "Updating learning rate to 3.58020718828878e-06\n",
      "epoch: 74 MSE loss: 0.1351 MSE valid loss: 0.1009\n",
      "Updating learning rate to 3.5565799981359997e-06\n",
      "epoch: 75 MSE loss: 0.1350 MSE valid loss: 0.1011\n",
      "Updating learning rate to 3.5334144928703013e-06\n",
      "epoch: 76 MSE loss: 0.1342 MSE valid loss: 0.1014\n",
      "Updating learning rate to 3.5106958298915256e-06\n",
      "epoch: 77 MSE loss: 0.1351 MSE valid loss: 0.1011\n",
      "Updating learning rate to 3.4884098261621723e-06\n",
      "epoch: 78 MSE loss: 0.1352 MSE valid loss: 0.1014\n",
      "Updating learning rate to 3.466542920996499e-06\n",
      "epoch: 79 MSE loss: 0.1344 MSE valid loss: 0.1015\n",
      "Updating learning rate to 3.4450821413837328e-06\n",
      "epoch: 80 MSE loss: 0.1338 MSE valid loss: 0.1015\n",
      "Updating learning rate to 3.4240150696439336e-06\n",
      "epoch: 81 MSE loss: 0.1339 MSE valid loss: 0.1015\n",
      "Updating learning rate to 3.4033298132333132e-06\n",
      "epoch: 82 MSE loss: 0.1338 MSE valid loss: 0.1019\n",
      "Updating learning rate to 3.383014976532195e-06\n",
      "epoch: 83 MSE loss: 0.1340 MSE valid loss: 0.1022\n",
      "Updating learning rate to 3.3630596344635904e-06\n",
      "epoch: 84 MSE loss: 0.1334 MSE valid loss: 0.1022\n",
      "Updating learning rate to 3.3434533078036348e-06\n",
      "epoch: 85 MSE loss: 0.1333 MSE valid loss: 0.1024\n",
      "Updating learning rate to 3.3241859400571367e-06\n",
      "epoch: 86 MSE loss: 0.1334 MSE valid loss: 0.1026\n",
      "Updating learning rate to 3.305247875782325e-06\n",
      "epoch: 87 MSE loss: 0.1327 MSE valid loss: 0.1027\n",
      "Updating learning rate to 3.2866298402586612e-06\n",
      "epoch: 88 MSE loss: 0.1332 MSE valid loss: 0.1028\n",
      "Updating learning rate to 3.2683229204004673e-06\n",
      "epoch: 89 MSE loss: 0.1341 MSE valid loss: 0.1024\n",
      "Updating learning rate to 3.2503185468271486e-06\n",
      "epoch: 90 MSE loss: 0.1329 MSE valid loss: 0.1029\n",
      "Updating learning rate to 3.232608477008077e-06\n",
      "epoch: 91 MSE loss: 0.1327 MSE valid loss: 0.1030\n",
      "Updating learning rate to 3.2151847794068398e-06\n",
      "epoch: 92 MSE loss: 0.1324 MSE valid loss: 0.1030\n",
      "Updating learning rate to 3.1980398185555676e-06\n",
      "epoch: 93 MSE loss: 0.1334 MSE valid loss: 0.1035\n",
      "Updating learning rate to 3.1811662409955475e-06\n",
      "epoch: 94 MSE loss: 0.1325 MSE valid loss: 0.1031\n",
      "Updating learning rate to 3.1645569620253167e-06\n",
      "epoch: 95 MSE loss: 0.1322 MSE valid loss: 0.1034\n",
      "Updating learning rate to 3.1482051532020014e-06\n",
      "epoch: 96 MSE loss: 0.1321 MSE valid loss: 0.1032\n",
      "Updating learning rate to 3.1321042305458136e-06\n",
      "epoch: 97 MSE loss: 0.1319 MSE valid loss: 0.1037\n",
      "Updating learning rate to 3.116247843401426e-06\n",
      "epoch: 98 MSE loss: 0.1323 MSE valid loss: 0.1038\n",
      "Updating learning rate to 3.1006298639134353e-06\n",
      "epoch: 99 MSE loss: 0.1317 MSE valid loss: 0.1038\n",
      "Elapsed Time: 219.5989857329987\n"
     ]
    }
   ],
   "source": [
    "c_in = 7\n",
    "context_window = 336\n",
    "target_window = 96\n",
    "patch_len = 16\n",
    "stride = 8 \n",
    "\n",
    "patchtst_model = PatchTST(c_in=c_in, context_window=context_window, target_window=target_window, patch_len=patch_len, stride=stride)\n",
    "\n",
    "Linear_model = Linear(c_in=c_in, context_window=context_window, target_window=target_window)\n",
    "DLinear_model = DLinear(c_in=c_in, context_window=context_window, target_window=target_window)\n",
    "\n",
    "Linear_learner = Learner_2(model=Linear_model, dataset=ETTDataset, adjust_lr=True, adjust_factor=0.01)\n",
    "DLinear_learner = Learner_2(model=DLinear_model, dataset=ETTDataset, adjust_lr=True, adjust_factor=0.01)\n",
    "patchtst_learner = Learner(model=patchtst_model, dataset=ETTDataset, adjust_lr=True, adjust_factor=0.001)\n",
    "\n",
    "Linear_train_history, Linear_valid_history = Linear_learner.train()\n",
    "DLinear_train_history, DLinear_valid_history = DLinear_learner.train()\n",
    "# import torch.autograd.profiler as profiler\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "# with profiler.profile(record_shapes=True, use_cuda=True) as prof:\n",
    "patchtst_train_history, patchtst_valid_history = patchtst_learner.train()\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(\"Elapsed Time:\", elapsed_time)\n",
    "# print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "# print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "ax.plot(np.arange(1, Linear_learner.epochs+1), Linear_valid_history, label=\"Linear\")\n",
    "ax.plot(np.arange(1, DLinear_learner.epochs+1), DLinear_valid_history, label=\"DLinear\")\n",
    "ax.plot(np.arange(1, patchtst_learner.epochs+1), patchtst_valid_history, label=\"PatchTST\")\n",
    "ax.set_xlabel(\"epochs\")\n",
    "ax.set_ylabel(\"MSE Error\")\n",
    "ax.set_ylim(0, 1.5)\n",
    "ax.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 3.096MB\n"
     ]
    }
   ],
   "source": [
    "model = patchtst_model\n",
    "param_size = 0\n",
    "for param in model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('model size: {:.3f}MB'.format(size_all_mb))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
