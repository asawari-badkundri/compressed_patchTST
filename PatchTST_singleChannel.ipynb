{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch import Tensor\n",
    "from typing import Callable, Optional\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  date   HUFL   HULL   MUFL   MULL   LUFL   LULL         OT\n",
      "0  2016-07-01 00:00:00  5.827  2.009  1.599  0.462  4.203  1.340  30.531000\n",
      "1  2016-07-01 01:00:00  5.693  2.076  1.492  0.426  4.142  1.371  27.787001\n",
      "2  2016-07-01 02:00:00  5.157  1.741  1.279  0.355  3.777  1.218  27.787001\n",
      "3  2016-07-01 03:00:00  5.090  1.942  1.279  0.391  3.807  1.279  25.044001\n",
      "4  2016-07-01 04:00:00  5.358  1.942  1.492  0.462  3.868  1.279  21.948000\n"
     ]
    }
   ],
   "source": [
    "df_ETTh1 = pd.read_csv(\"/scratch/ab10445/hpml/PatchTST/dataset/ETT-small/ETTh1.csv\")\n",
    "print(df_ETTh1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17420, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ETTh1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset=\"ETTh1\", mode=\"train\", scale=True, seq_len=336, pred_len=96):\n",
    "        super().__init__()\n",
    "        df = pd.read_csv(\"/scratch/ab10445/hpml/PatchTST/dataset/ETT-small/ETTh1.csv\".format(dataset))\n",
    "        x_y = df.iloc[:,1:]\n",
    "        time_stamp = df.iloc[:,0]\n",
    "\n",
    "        assert mode in ['train', 'test', 'val']\n",
    "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
    "        self.set_type = type_map[mode]\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "        border1s = [0, 12 * 30 * 24 - self.seq_len, 12 * 30 * 24 + 4 * 30 * 24 - self.seq_len]\n",
    "        border2s = [12 * 30 * 24, 12 * 30 * 24 + 4 * 30 * 24, 12 * 30 * 24 + 8 * 30 * 24]\n",
    "        border1 = border1s[self.set_type]\n",
    "        border2 = border2s[self.set_type]\n",
    "\n",
    "        if scale:\n",
    "            train_x_y = x_y.iloc[border1s[0]: border2s[0]]\n",
    "            self.ss = StandardScaler()\n",
    "            self.ss.fit(train_x_y.to_numpy(dtype=np.float32))\n",
    "            x_y = self.ss.transform(x_y.to_numpy(dtype=np.float32))\n",
    "        else:\n",
    "            x_y = x_y.to_numpy(dtype=np.float32)\n",
    "        \n",
    "        time_stamp = time_stamp.to_numpy()     \n",
    "        \n",
    "        self.data_x = x_y[border1: border2, :]\n",
    "        self.data_y = x_y[border1: border2, -1]\n",
    "\n",
    "        self.data_stamp = time_stamp[border1: border2]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end\n",
    "        r_end = r_begin + self.pred_len\n",
    "\n",
    "        seq_x = self.data_x[s_begin:s_end]\n",
    "        seq_y = self.data_y[r_begin:r_end]\n",
    "        return seq_x, seq_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x) - self.seq_len - self.pred_len + 1\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return self.ss.inverse_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementation of utils\n",
    "class RevIN(torch.nn.Module):\n",
    "    def __init__(self, num_features: int, eps=1e-5, affine=True, subtract_last=False, target_idx=-1):\n",
    "        super(RevIN, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "        self.subtract_last = subtract_last\n",
    "        #target idx added\n",
    "        self.target_idx = target_idx\n",
    "        if self.affine:\n",
    "            self._init_params()\n",
    "\n",
    "    def forward(self, x, mode):\n",
    "        if mode == \"norm\":\n",
    "            self._get_statistics(x)\n",
    "            x = self._normalize(x)\n",
    "        elif mode == \"denorm\":\n",
    "            x = self._denormalize(x)\n",
    "        else: raise AssertionError\n",
    "        return x\n",
    "\n",
    "    def _init_params(self):\n",
    "        self.affine_weight = torch.nn.Parameter(torch.ones(self.num_features))\n",
    "        self.affine_bias = torch.nn.Parameter(torch.zeros(self.num_features))\n",
    "\n",
    "    def _get_statistics(self, x):\n",
    "        dim2reduce = tuple(range(1, x.ndim-1))\n",
    "        if self.subtract_last:\n",
    "            self.last = x[:,-1,:].unsqueeze(1)\n",
    "        else:\n",
    "            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
    "        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n",
    "\n",
    "    def _normalize(self, x):\n",
    "        if self.subtract_last:\n",
    "            x = x - self.last\n",
    "        else:\n",
    "            x = x - self.mean\n",
    "        x = x / self.stdev\n",
    "        if self.affine:\n",
    "            print(f'x.shape: {x.shape} | self.affine_weight: {self.affine_weight.shape}')\n",
    "            x = x * self.affine_weight\n",
    "            x = x + self.affine_bias\n",
    "        return x\n",
    "\n",
    "    def _denormalize(self, x):\n",
    "        if self.affine:\n",
    "            x = x - self.affine_bias\n",
    "            x = x / (self.affine_weight + self.eps*self.eps)\n",
    "        # x = x * self.stdev\n",
    "        # if self.subtract_last:\n",
    "        #     x = x + self.last\n",
    "        # else:\n",
    "        #     x = x + self.mean\n",
    "        # return x\n",
    "        x = x * self.stdev[:, :, self.target_idx]\n",
    "        if self.subtract_last:\n",
    "            x = x + self.last[:, :, self.target_idx]\n",
    "        else:\n",
    "            x = x + self.mean[:, :, self.target_idx]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transpose(torch.nn.Module):\n",
    "    def __init__(self, *dims, contiguous=False): \n",
    "        super().__init__()\n",
    "        self.dims, self.contiguous = dims, contiguous\n",
    "    def forward(self, x):\n",
    "        if self.contiguous: \n",
    "            return x.transpose(*self.dims).contiguous()\n",
    "        else: \n",
    "            return x.transpose(*self.dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(q_len, d_model):\n",
    "    W_pos = torch.empty((q_len, d_model))\n",
    "    torch.nn.init.uniform_(W_pos, -0.02, 0.02)\n",
    "    return torch.nn.Parameter(W_pos, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation of PatchTST Encorder layer\n",
    "class TSTiEncoder(torch.nn.Module):  #i means channel-independent\n",
    "    def __init__(self, c_in, patch_num, patch_len, max_seq_len=1024,\n",
    "                 n_layers=3, d_model=128, n_heads=16, d_k=None, d_v=None,\n",
    "                 d_ff=256, norm='BatchNorm', attn_dropout=0., dropout=0., store_attn=False,\n",
    "                 key_padding_mask='auto', padding_var=None, attn_mask=None, res_attention=True, pre_norm=False,\n",
    "                 verbose=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.patch_num = patch_num\n",
    "        self.patch_len = patch_len\n",
    "        # Input encoding\n",
    "        q_len = patch_num\n",
    "        self.W_P = torch.nn.Linear(patch_len, d_model)        # Eq 1: projection of feature vectors onto a d-dim vector space\n",
    "        self.seq_len = q_len\n",
    "        # Positional encoding\n",
    "        self.W_pos = positional_encoding(q_len, d_model)\n",
    "        # Residual dropout\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        # Encoder\n",
    "        self.encoder = TSTEncoder(q_len, d_model, n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm, attn_dropout=attn_dropout, dropout=dropout,\n",
    "                                   pre_norm=pre_norm, res_attention=res_attention, n_layers=n_layers, store_attn=store_attn)\n",
    "\n",
    "    def forward(self, x):                                              # x: [bs x nvars x patch_len x patch_num]\n",
    "        \n",
    "        n_vars = x.shape[1]\n",
    "        # Input encoding\n",
    "        x = x.permute(0,1,3,2)                                                   # x: [bs x nvars x patch_num x patch_len]\n",
    "        x = self.W_P(x)                                                          # x: [bs x nvars x patch_num x d_model]\n",
    "\n",
    "        u = torch.reshape(x, (x.shape[0]*x.shape[1],x.shape[2],x.shape[3]))      # u: [bs * nvars x patch_num x d_model]\n",
    "        u = self.dropout(u + self.W_pos)                                         # u: [bs * nvars x patch_num x d_model]\n",
    "\n",
    "        # Encoder\n",
    "        z = self.encoder(u)                                                      # z: [bs * nvars x patch_num x d_model]\n",
    "        z = torch.reshape(z, (-1,n_vars,z.shape[-2],z.shape[-1]))                # z: [bs x nvars x patch_num x d_model]\n",
    "        z = z.permute(0,1,3,2)                                                   # z: [bs x nvars x d_model x patch_num]\n",
    "        \n",
    "        return z   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSTEncoder(torch.nn.Module):\n",
    "    def __init__(self, q_len, d_model, n_heads, d_k=None, d_v=None, d_ff=None, \n",
    "                        norm='BatchNorm', attn_dropout=0., dropout=0., \n",
    "                        res_attention=False, n_layers=1, pre_norm=False, store_attn=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = torch.nn.ModuleList([TSTEncoderLayer(q_len, d_model, n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff, norm=norm,\n",
    "                                                      attn_dropout=attn_dropout, dropout=dropout,\n",
    "                                                      res_attention=res_attention,\n",
    "                                                      pre_norm=pre_norm, store_attn=store_attn) for i in range(n_layers)])\n",
    "        self.res_attention = res_attention\n",
    "\n",
    "    def forward(self, src, key_padding_mask=None, attn_mask=None):\n",
    "        output = src\n",
    "        scores = None\n",
    "        if self.res_attention:\n",
    "            for mod in self.layers: \n",
    "                output, scores = mod(output, prev=scores, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "            return output\n",
    "        else:\n",
    "            for mod in self.layers: \n",
    "                output = mod(output, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSTEncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, q_len, d_model, n_heads, d_k=None, d_v=None, d_ff=256, store_attn=False,\n",
    "                 norm='BatchNorm', attn_dropout=0, dropout=0., bias=True, res_attention=False, pre_norm=False):\n",
    "        super().__init__()\n",
    "        assert not d_model%n_heads, f\"d_model ({d_model}) must be divisible by n_heads ({n_heads})\"\n",
    "        d_k = d_model // n_heads if d_k is None else d_k\n",
    "        d_v = d_model // n_heads if d_v is None else d_v\n",
    "\n",
    "        # Multi-Head attention\n",
    "        self.res_attention = res_attention\n",
    "        self.self_attn = _MultiheadAttention(d_model, n_heads, d_k, d_v, attn_dropout=attn_dropout, proj_dropout=dropout, res_attention=res_attention)\n",
    "\n",
    "        # Add & Norm\n",
    "        self.dropout_attn = torch.nn.Dropout(dropout)\n",
    "        if \"batch\" in norm.lower():\n",
    "            self.norm_attn = torch.nn.Sequential(Transpose(1,2), torch.nn.BatchNorm1d(d_model), Transpose(1,2))\n",
    "        else:\n",
    "            self.norm_attn = torch.nn.LayerNorm(d_model)\n",
    "        # self.ff = nn.Sequential(nn.Linear(d_model, d_ff, bias=bias),\n",
    "        #                         get_activation_fn(activation),\n",
    "        #                         nn.Dropout(dropout),\n",
    "        #                         nn.Linear(d_ff, d_model, bias=bias))\n",
    "        # Position-wise Feed-Forward\n",
    "        self.ff = torch.nn.Sequential(torch.nn.Linear(d_model, d_ff, bias=bias),\n",
    "                                torch.nn.GELU(),\n",
    "                                torch.nn.Dropout(dropout),\n",
    "                                torch.nn.Linear(d_ff, d_model, bias=bias))\n",
    "\n",
    "        # Add & Norm\n",
    "        self.dropout_ffn = torch.nn.Dropout(dropout)\n",
    "        if \"batch\" in norm.lower():\n",
    "            self.norm_ffn = torch.nn.Sequential(Transpose(1,2), torch.nn.BatchNorm1d(d_model), Transpose(1,2))\n",
    "        else:\n",
    "            self.norm_ffn = torch.nn.LayerNorm(d_model)\n",
    "\n",
    "        self.pre_norm = pre_norm\n",
    "        self.store_attn = store_attn\n",
    "\n",
    "    def forward(self, src, prev=None, key_padding_mask=None, attn_mask=None):\n",
    "\n",
    "        # Multi-Head attention sublayer\n",
    "        if self.pre_norm:\n",
    "            src = self.norm_attn(src)\n",
    "        ## Multi-Head attention\n",
    "        if self.res_attention:\n",
    "            src2, attn, scores = self.self_attn(src, src, src, prev, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "        else:\n",
    "            src2, attn = self.self_attn(src, src, src, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "        if self.store_attn:\n",
    "            self.attn = attn\n",
    "        ## Add & Norm\n",
    "        src = src + self.dropout_attn(src2) # Add: residual connection with residual dropout\n",
    "        if not self.pre_norm:\n",
    "            src = self.norm_attn(src)\n",
    "\n",
    "        # Feed-forward sublayer\n",
    "        if self.pre_norm:\n",
    "            src = self.norm_ffn(src)\n",
    "        ## Position-wise Feed-Forward\n",
    "        src2 = self.ff(src)\n",
    "        ## Add & Norm\n",
    "        src = src + self.dropout_ffn(src2) # Add: residual connection with residual dropout\n",
    "        if not self.pre_norm:\n",
    "            src = self.norm_ffn(src)\n",
    "\n",
    "        if self.res_attention:\n",
    "            return src, scores\n",
    "        else:\n",
    "            return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _MultiheadAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_k=None, d_v=None, res_attention=False, attn_dropout=0., proj_dropout=0., qkv_bias=True, lsa=False):\n",
    "        super().__init__()\n",
    "        d_k = d_model // n_heads if d_k is None else d_k\n",
    "        d_v = d_model // n_heads if d_v is None else d_v\n",
    "\n",
    "        self.n_heads, self.d_k, self.d_v = n_heads, d_k, d_v\n",
    "\n",
    "        self.W_Q = torch.nn.Linear(d_model, d_k * n_heads, bias=qkv_bias)\n",
    "        self.W_K = torch.nn.Linear(d_model, d_k * n_heads, bias=qkv_bias)\n",
    "        self.W_V = torch.nn.Linear(d_model, d_v * n_heads, bias=qkv_bias)\n",
    "\n",
    "        # Scaled Dot-Product Attention (multiple heads)\n",
    "        self.res_attention = res_attention\n",
    "        self.sdp_attn = _ScaledDotProductAttention(d_model, n_heads, attn_dropout=attn_dropout, res_attention=self.res_attention, lsa=lsa)\n",
    "\n",
    "        # Poject output\n",
    "        self.to_out = torch.nn.Sequential(torch.nn.Linear(n_heads * d_v, d_model), torch.nn.Dropout(proj_dropout))\n",
    "\n",
    "    def forward(self, Q, K=None, V=None, prev=None,\n",
    "                key_padding_mask=None, attn_mask=None):\n",
    "\n",
    "        bs = Q.size(0)\n",
    "        if K is None: K = Q\n",
    "        if V is None: V = Q\n",
    "\n",
    "        # Linear (+ split in multiple heads)\n",
    "        q_s = self.W_Q(Q).view(bs, -1, self.n_heads, self.d_k).transpose(1,2)       # q_s    : [bs x n_heads x max_q_len x d_k]\n",
    "        k_s = self.W_K(K).view(bs, -1, self.n_heads, self.d_k).permute(0,2,3,1)     # k_s    : [bs x n_heads x d_k x q_len] - transpose(1,2) + transpose(2,3)\n",
    "        v_s = self.W_V(V).view(bs, -1, self.n_heads, self.d_v).transpose(1,2)       # v_s    : [bs x n_heads x q_len x d_v]\n",
    "\n",
    "        # Apply Scaled Dot-Product Attention (multiple heads)\n",
    "        if self.res_attention:\n",
    "            output, attn_weights, attn_scores = self.sdp_attn(q_s, k_s, v_s, prev=prev, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "        else:\n",
    "            output, attn_weights = self.sdp_attn(q_s, k_s, v_s, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "        # output: [bs x n_heads x q_len x d_v], attn: [bs x n_heads x q_len x q_len], scores: [bs x n_heads x max_q_len x q_len]\n",
    "\n",
    "        # back to the original inputs dimensions\n",
    "        output = output.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * self.d_v) # output: [bs x q_len x n_heads * d_v]\n",
    "        output = self.to_out(output)\n",
    "\n",
    "        if self.res_attention: return output, attn_weights, attn_scores\n",
    "        else: return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _ScaledDotProductAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_heads, attn_dropout=0., res_attention=False, lsa=False):\n",
    "        super().__init__()\n",
    "        self.attn_dropout = torch.nn.Dropout(attn_dropout)\n",
    "        self.res_attention = res_attention\n",
    "        head_dim = d_model // n_heads\n",
    "        self.scale = torch.nn.Parameter(torch.tensor(head_dim ** -0.5), requires_grad=lsa)\n",
    "        self.lsa = lsa\n",
    "\n",
    "    def forward(self, q, k, v, prev=None, key_padding_mask=None, attn_mask=None):\n",
    "        # Scaled MatMul (q, k) - similarity scores for all pairs of positions in an input sequence\n",
    "        attn_scores = torch.matmul(q, k) * self.scale      # attn_scores : [bs x n_heads x max_q_len x q_len]\n",
    "\n",
    "        # Add pre-softmax attention scores from the previous layer (optional)\n",
    "        if prev is not None: attn_scores = attn_scores + prev\n",
    "\n",
    "        # Attention mask (optional)\n",
    "        if attn_mask is not None:                                     # attn_mask with shape [q_len x seq_len] - only used when q_len == seq_len\n",
    "            if attn_mask.dtype == torch.bool:\n",
    "                attn_scores.masked_fill_(attn_mask, -np.inf)\n",
    "            else:\n",
    "                attn_scores += attn_mask\n",
    "\n",
    "        # Key padding mask (optional)\n",
    "        if key_padding_mask is not None:                              # mask with shape [bs x q_len] (only when max_w_len == q_len)\n",
    "            attn_scores.masked_fill_(key_padding_mask.unsqueeze(1).unsqueeze(2), -np.inf)\n",
    "\n",
    "        # normalize the attention weights\n",
    "        attn_weights = torch.nn.functional.softmax(attn_scores, dim=-1)                 # attn_weights   : [bs x n_heads x max_q_len x q_len]\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "\n",
    "        # compute the new values given the attention weights\n",
    "        output = torch.matmul(attn_weights, v)                        # output: [bs x n_heads x max_q_len x d_v]\n",
    "\n",
    "        if self.res_attention: return output, attn_weights, attn_scores\n",
    "        else: return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten_Head(torch.nn.Module):\n",
    "    def __init__(self, n_vars, nf, target_window, head_dropout=0):\n",
    "        super().__init__()\n",
    "        self.n_vars = n_vars\n",
    "        self.flatten = torch.nn.Flatten(start_dim=-3) #flattens last 3\n",
    "        self.linear = torch.nn.Linear(nf * n_vars, target_window) \n",
    "        self.dropout = torch.nn.Dropout(head_dropout)\n",
    "            \n",
    "    def forward(self, x):                                 # x: [bs x nvars x d_model x patch_num]\n",
    "        x = self.flatten(x)                               # x: [bs x nvars * d_model * patch_num]\n",
    "        x = self.linear(x)                                # x: [bs x target_window]\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchTST(torch.nn.Module):\n",
    "    def __init__(self, c_in, context_window, target_window, patch_len, stride, max_seq_len=1024, \n",
    "                 n_layers=3, d_model=16, n_heads=4, d_k=None, d_v=None,\n",
    "                 d_ff=128, attn_dropout=0.0, dropout=0.3, key_padding_mask='auto',\n",
    "                 padding_var=None, attn_mask=None, res_attention=True, pre_norm=False, store_attn=False,\n",
    "                 head_dropout = 0.0, padding_patch = \"end\",\n",
    "                 revin = True, affine = False, subtract_last = False,\n",
    "                 verbose=False, target_idx=-1, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.revin = revin\n",
    "        if revin:\n",
    "            self.revin_layer = RevIN(c_in, affine=affine, subtract_last=subtract_last, target_idx=target_idx)\n",
    "\n",
    "        self.patch_len = patch_len\n",
    "        self.stride = stride\n",
    "        self.padding_patch = padding_patch\n",
    "        patch_num = int((context_window - patch_len)/stride + 1)\n",
    "\n",
    "        if padding_patch == \"end\":\n",
    "            self.padding_patch_layer = torch.nn.ReplicationPad1d((0, stride))\n",
    "            patch_num += 1\n",
    "\n",
    "        self.backbone = TSTiEncoder(c_in, patch_num=patch_num, patch_len=patch_len, max_seq_len=max_seq_len,\n",
    "                                n_layers=n_layers, d_model=d_model, n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff,\n",
    "                                attn_dropout=attn_dropout, dropout=dropout, key_padding_mask=key_padding_mask, padding_var=padding_var,\n",
    "                                attn_mask=attn_mask, res_attention=res_attention, pre_norm=pre_norm, store_attn=store_attn,\n",
    "                                verbose=verbose, **kwargs)\n",
    "        \n",
    "        self.head_nf = d_model * patch_num\n",
    "        self.n_vars = c_in\n",
    "\n",
    "        self.head = Flatten_Head(self.n_vars, self.head_nf, target_window, head_dropout=head_dropout)\n",
    "        \n",
    "    def forward(self, z):                                                                   # z: [bs x seq_len × nvars]\n",
    "        # instance norm\n",
    "        if self.revin:                                                        \n",
    "            z = self.revin_layer(z, 'norm')\n",
    "            z = z.permute(0,2,1)                                                            # z: [bs x nvars × seq_len]\n",
    "            \n",
    "        # do patching\n",
    "        if self.padding_patch == 'end':\n",
    "            z = self.padding_patch_layer(z)\n",
    "        # print(f'z.shape: {z.shape}')\n",
    "        z = z.unfold(dimension=-1, size=self.patch_len, step=self.stride)                   # z: [bs x nvars x patch_num x patch_len]\n",
    "        z = z.permute(0,1,3,2)                                                              # z: [bs x nvars x patch_len x patch_num]\n",
    "        \n",
    "        # model\n",
    "        z = self.backbone(z)                                                                # z: [bs x nvars x d_model x patch_num]\n",
    "        z = self.head(z)                                                                    # z: [bs x target_window] \n",
    "        \n",
    "        # denorm\n",
    "        if self.revin:                                                        \n",
    "            z = self.revin_layer(z, 'denorm')\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear model\n",
    "class Linear(torch.nn.Module):\n",
    "    def __init__(self, c_in, context_window, target_window):\n",
    "        super().__init__()\n",
    "        self.c_in = c_in\n",
    "        self.context_winsoq = context_window\n",
    "        self.target_window = target_window\n",
    "\n",
    "        self.flatten = torch.nn.Flatten(start_dim=-2)\n",
    "\n",
    "        self.linear = torch.nn.Linear(c_in * context_window, target_window)\n",
    "    \n",
    "    def forward(self, x):                   # x: [bs x seq_len × nvars]\n",
    "        x = self.flatten(x)                 # x: [bs x seq_len * nvars]\n",
    "        x = self.linear(x)                  # x: [bs x target_window]\n",
    "        return x\n",
    "    \n",
    "\n",
    "class moving_avg(torch.nn.Module):\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = torch.nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # padding on the both ends of time series\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class series_decomp(torch.nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "        return res, moving_mean\n",
    "\n",
    "class DLinear(torch.nn.Module):\n",
    "    def __init__(self, c_in, context_window, target_window):\n",
    "        super().__init__()\n",
    "        # Decompsition Kernel Size\n",
    "        kernel_size = 25\n",
    "        self.decompsition = series_decomp(kernel_size)\n",
    "        self.flatten_Seasonal = torch.nn.Flatten(start_dim=-2)\n",
    "        self.flatten_Trend = torch.nn.Flatten(start_dim=-2)\n",
    "        \n",
    "        self.Linear_Seasonal = torch.nn.Linear(c_in * context_window, target_window)\n",
    "        self.Linear_Trend = torch.nn.Linear(c_in * context_window, target_window)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [Batch, Input length, Channel]\n",
    "        seasonal_init, trend_init = self.decompsition(x)\n",
    "        seasonal_init = self.flatten_Seasonal(x)\n",
    "        trend_init = self.flatten_Trend(x)\n",
    "\n",
    "        seasonal_output = self.Linear_Seasonal(seasonal_init)\n",
    "        trend_output = self.Linear_Trend(trend_init)\n",
    "\n",
    "        x = seasonal_output + trend_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self, model, dataset, batch_size=128, lr=0.0001, epochs=100, target_window=96, d_model=16, adjust_lr=True, adjust_factor=0.001):\n",
    "        self.model = model.to(\"cuda\")\n",
    "        self.batch_size = batch_size\n",
    "        train_dataset = dataset(mode=\"train\")\n",
    "        valid_dataset = dataset(mode=\"val\")\n",
    "        test_dataset = dataset(mode=\"test\")\n",
    "        self.train_datalen = len(train_dataset)\n",
    "        self.valid_datalen = len(valid_dataset)\n",
    "        self.train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "        self.test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        self.lr=lr\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.loss = torch.nn.MSELoss()\n",
    "        self.epochs = epochs\n",
    "        self.target_window=target_window\n",
    "        self.best_weight = self.model.state_dict()\n",
    "        self.d_model=d_model\n",
    "        self.adjust_lr = adjust_lr\n",
    "        self.adjust_factor = adjust_factor\n",
    "    \n",
    "    def adjust_learning_rate(self, steps, warmup_step=300, printout=True):\n",
    "        if steps**(-0.5) < steps * (warmup_step**-1.5):\n",
    "            lr_adjust = (16**-0.5) * (steps**-0.5) * self.adjust_factor\n",
    "        else:\n",
    "            lr_adjust = (16**-0.5) * (steps * (warmup_step**-1.5)) * self.adjust_factor\n",
    "\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr_adjust\n",
    "        if printout: \n",
    "            print('Updating learning rate to {}'.format(lr_adjust))\n",
    "        return \n",
    "\n",
    "    def train(self):\n",
    "        best_valid_loss = np.inf\n",
    "        train_history = []\n",
    "        valid_history = []\n",
    "        train_steps = 1\n",
    "        if self.adjust_lr:\n",
    "            self.adjust_learning_rate(train_steps)\n",
    "        for epoch in range(self.epochs):\n",
    "            #train\n",
    "            self.model.train()\n",
    "            iter_count = 0\n",
    "            total_loss = 0\n",
    "\n",
    "            for train_x, train_y in self.train_dataloader:\n",
    "                train_x = train_x.to(\"cuda\")\n",
    "                train_y = train_y.to(\"cuda\")\n",
    "\n",
    "                pred_y = self.model(train_x)\n",
    "                loss = self.loss(pred_y, train_y)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                iter_count += 1\n",
    "                train_steps += 1\n",
    "            if self.adjust_lr:\n",
    "                self.adjust_learning_rate(train_steps)\n",
    "\n",
    "            #valid\n",
    "            self.model.eval()\n",
    "            valid_iter_count = 0\n",
    "            valid_total_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for valid_x, valid_y in self.valid_dataloader:\n",
    "                    valid_x = valid_x.to(\"cuda\")\n",
    "                    valid_y = valid_y.to(\"cuda\")\n",
    "                    pred_y = self.model(valid_x)\n",
    "                    loss = self.loss(pred_y, valid_y)\n",
    "                    valid_total_loss += loss.item()\n",
    "                    valid_iter_count += 1\n",
    "            \n",
    "            total_loss /= iter_count\n",
    "            valid_total_loss /= valid_iter_count\n",
    "            print(\"epoch: {} MSE loss: {:.4f} MSE valid loss: {:.4f}\".format(epoch, total_loss, valid_total_loss))\n",
    "            if best_valid_loss >= valid_total_loss:\n",
    "                self.best_weight = self.model.state_dict()\n",
    "                best_valid_loss = valid_total_loss\n",
    "                # print(\"Best score! Weights of the model are updated!\")\n",
    "            train_history.append(total_loss)\n",
    "            valid_history.append(valid_total_loss)\n",
    "        return train_history, valid_history\n",
    "\n",
    "    def test(self):\n",
    "        self.model.load_state_dict(self.best_weight)\n",
    "        self.model.eval()\n",
    "        iter_count = 0\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for test_x, test_y in self.test_dataloader:\n",
    "                test_x = test_x.to(\"cuda\")\n",
    "                test_y = test_y.to(\"cuda\")\n",
    "                pred_y = self.model(test_x)\n",
    "                loss = self.loss(pred_y, test_y)\n",
    "                total_loss += loss.item()\n",
    "                iter_count += 1\n",
    "        total_loss /= iter_count\n",
    "        print(\"MSE test loss: {:.4f}\".format(total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 4.811252243246882e-07\n",
      "Updating learning rate to 3.1754264805429416e-05\n",
      "epoch: 0 MSE loss: 1.3168 MSE valid loss: 0.8544\n",
      "Updating learning rate to 6.302740438653415e-05\n",
      "epoch: 1 MSE loss: 0.6125 MSE valid loss: 0.7043\n",
      "Updating learning rate to 9.430054396763888e-05\n",
      "epoch: 2 MSE loss: 0.3193 MSE valid loss: 0.5746\n",
      "Updating learning rate to 0.00012557368354874362\n",
      "epoch: 3 MSE loss: 0.2475 MSE valid loss: 0.4325\n",
      "Updating learning rate to 0.00013846219390542782\n",
      "epoch: 4 MSE loss: 0.2094 MSE valid loss: 0.3239\n",
      "Updating learning rate to 0.0001264304343560434\n",
      "epoch: 5 MSE loss: 0.1890 MSE valid loss: 0.2729\n",
      "Updating learning rate to 0.00011707322644771175\n",
      "epoch: 6 MSE loss: 0.1771 MSE valid loss: 0.2591\n",
      "Updating learning rate to 0.00010952698858458088\n",
      "epoch: 7 MSE loss: 0.1689 MSE valid loss: 0.2567\n",
      "Updating learning rate to 0.00010327404809650345\n",
      "epoch: 8 MSE loss: 0.1636 MSE valid loss: 0.2573\n",
      "Updating learning rate to 9.798272520870258e-05\n",
      "epoch: 9 MSE loss: 0.1597 MSE valid loss: 0.2551\n",
      "Updating learning rate to 9.342938659399198e-05\n",
      "epoch: 10 MSE loss: 0.1559 MSE valid loss: 0.2590\n",
      "Updating learning rate to 8.945703337056415e-05\n",
      "epoch: 11 MSE loss: 0.1533 MSE valid loss: 0.2573\n",
      "Updating learning rate to 8.595177052156611e-05\n",
      "epoch: 12 MSE loss: 0.1516 MSE valid loss: 0.2512\n",
      "Updating learning rate to 8.282869524032146e-05\n",
      "epoch: 13 MSE loss: 0.1491 MSE valid loss: 0.2550\n",
      "Updating learning rate to 8.002304995805999e-05\n",
      "epoch: 14 MSE loss: 0.1475 MSE valid loss: 0.2566\n",
      "Updating learning rate to 7.748446592171796e-05\n",
      "epoch: 15 MSE loss: 0.1452 MSE valid loss: 0.2512\n",
      "Updating learning rate to 7.517309741553296e-05\n",
      "epoch: 16 MSE loss: 0.1441 MSE valid loss: 0.2514\n",
      "Updating learning rate to 7.305695402335044e-05\n",
      "epoch: 17 MSE loss: 0.1430 MSE valid loss: 0.2531\n",
      "Updating learning rate to 7.111001549857179e-05\n",
      "epoch: 18 MSE loss: 0.1415 MSE valid loss: 0.2482\n",
      "Updating learning rate to 6.931087162517846e-05\n",
      "epoch: 19 MSE loss: 0.1411 MSE valid loss: 0.2524\n",
      "Updating learning rate to 6.76417225936176e-05\n",
      "epoch: 20 MSE loss: 0.1400 MSE valid loss: 0.2506\n",
      "Updating learning rate to 6.608763214317867e-05\n",
      "epoch: 21 MSE loss: 0.1387 MSE valid loss: 0.2485\n",
      "Updating learning rate to 6.46359612493774e-05\n",
      "epoch: 22 MSE loss: 0.1381 MSE valid loss: 0.2483\n",
      "Updating learning rate to 6.327593294406921e-05\n",
      "epoch: 23 MSE loss: 0.1375 MSE valid loss: 0.2444\n",
      "Updating learning rate to 6.199829383043036e-05\n",
      "epoch: 24 MSE loss: 0.1374 MSE valid loss: 0.2460\n",
      "Updating learning rate to 6.079504788572469e-05\n",
      "epoch: 25 MSE loss: 0.1354 MSE valid loss: 0.2482\n",
      "Updating learning rate to 5.965924498791846e-05\n",
      "epoch: 26 MSE loss: 0.1360 MSE valid loss: 0.2486\n",
      "Updating learning rate to 5.8584811349126495e-05\n",
      "epoch: 27 MSE loss: 0.1338 MSE valid loss: 0.2463\n",
      "Updating learning rate to 5.7566412382313e-05\n",
      "epoch: 28 MSE loss: 0.1332 MSE valid loss: 0.2454\n",
      "Updating learning rate to 5.659934091583233e-05\n",
      "epoch: 29 MSE loss: 0.1329 MSE valid loss: 0.2453\n",
      "Updating learning rate to 5.567942539842175e-05\n",
      "epoch: 30 MSE loss: 0.1328 MSE valid loss: 0.2469\n",
      "Updating learning rate to 5.4802954002676805e-05\n",
      "epoch: 31 MSE loss: 0.1320 MSE valid loss: 0.2442\n",
      "Updating learning rate to 5.39666114720432e-05\n",
      "epoch: 32 MSE loss: 0.1309 MSE valid loss: 0.2418\n",
      "Updating learning rate to 5.316742625738918e-05\n",
      "epoch: 33 MSE loss: 0.1316 MSE valid loss: 0.2465\n",
      "Updating learning rate to 5.240272601878982e-05\n",
      "epoch: 34 MSE loss: 0.1309 MSE valid loss: 0.2418\n",
      "Updating learning rate to 5.1670099971819504e-05\n",
      "epoch: 35 MSE loss: 0.1301 MSE valid loss: 0.2435\n",
      "Updating learning rate to 5.096736686795811e-05\n",
      "epoch: 36 MSE loss: 0.1293 MSE valid loss: 0.2443\n",
      "Updating learning rate to 5.0292547639160534e-05\n",
      "epoch: 37 MSE loss: 0.1292 MSE valid loss: 0.2440\n",
      "Updating learning rate to 4.96438419243461e-05\n",
      "epoch: 38 MSE loss: 0.1284 MSE valid loss: 0.2442\n",
      "Updating learning rate to 4.901960784313725e-05\n",
      "epoch: 39 MSE loss: 0.1288 MSE valid loss: 0.2431\n",
      "Updating learning rate to 4.841834449897069e-05\n",
      "epoch: 40 MSE loss: 0.1272 MSE valid loss: 0.2429\n",
      "Updating learning rate to 4.783867678672252e-05\n",
      "epoch: 41 MSE loss: 0.1276 MSE valid loss: 0.2443\n",
      "Updating learning rate to 4.727934215451461e-05\n",
      "epoch: 42 MSE loss: 0.1268 MSE valid loss: 0.2425\n",
      "Updating learning rate to 4.6739179029417445e-05\n",
      "epoch: 43 MSE loss: 0.1269 MSE valid loss: 0.2419\n",
      "Updating learning rate to 4.621711666540848e-05\n",
      "epoch: 44 MSE loss: 0.1262 MSE valid loss: 0.2441\n",
      "Updating learning rate to 4.571216621155238e-05\n",
      "epoch: 45 MSE loss: 0.1258 MSE valid loss: 0.2410\n",
      "Updating learning rate to 4.522341283077635e-05\n",
      "epoch: 46 MSE loss: 0.1260 MSE valid loss: 0.2428\n",
      "Updating learning rate to 4.4750008726252546e-05\n",
      "epoch: 47 MSE loss: 0.1254 MSE valid loss: 0.2432\n",
      "Updating learning rate to 4.4291166954394496e-05\n",
      "epoch: 48 MSE loss: 0.1248 MSE valid loss: 0.2395\n",
      "Updating learning rate to 4.384615592171157e-05\n",
      "epoch: 49 MSE loss: 0.1246 MSE valid loss: 0.2417\n",
      "Updating learning rate to 4.341429447794924e-05\n",
      "epoch: 50 MSE loss: 0.1241 MSE valid loss: 0.2416\n",
      "Updating learning rate to 4.299494753063186e-05\n",
      "epoch: 51 MSE loss: 0.1242 MSE valid loss: 0.2383\n",
      "Updating learning rate to 4.2587522116770666e-05\n",
      "epoch: 52 MSE loss: 0.1244 MSE valid loss: 0.2415\n",
      "Updating learning rate to 4.219146387646126e-05\n",
      "epoch: 53 MSE loss: 0.1232 MSE valid loss: 0.2402\n",
      "Updating learning rate to 4.1806253880665696e-05\n",
      "epoch: 54 MSE loss: 0.1233 MSE valid loss: 0.2400\n",
      "Updating learning rate to 4.143140577189121e-05\n",
      "epoch: 55 MSE loss: 0.1232 MSE valid loss: 0.2427\n",
      "Updating learning rate to 4.106646318193315e-05\n",
      "epoch: 56 MSE loss: 0.1231 MSE valid loss: 0.2418\n",
      "Updating learning rate to 4.071099739550263e-05\n",
      "epoch: 57 MSE loss: 0.1225 MSE valid loss: 0.2436\n",
      "Updating learning rate to 4.0364605232539164e-05\n",
      "epoch: 58 MSE loss: 0.1226 MSE valid loss: 0.2486\n",
      "Updating learning rate to 4.0026907125422175e-05\n",
      "epoch: 59 MSE loss: 0.1219 MSE valid loss: 0.2419\n",
      "Updating learning rate to 3.969754537023144e-05\n",
      "epoch: 60 MSE loss: 0.1222 MSE valid loss: 0.2416\n",
      "Updating learning rate to 3.937618253373847e-05\n",
      "epoch: 61 MSE loss: 0.1217 MSE valid loss: 0.2430\n",
      "Updating learning rate to 3.90625e-05\n",
      "epoch: 62 MSE loss: 0.1213 MSE valid loss: 0.2399\n",
      "Updating learning rate to 3.875619664232189e-05\n",
      "epoch: 63 MSE loss: 0.1211 MSE valid loss: 0.2446\n",
      "Updating learning rate to 3.845698760800996e-05\n",
      "epoch: 64 MSE loss: 0.1206 MSE valid loss: 0.2402\n",
      "Updating learning rate to 3.816460320475954e-05\n",
      "epoch: 65 MSE loss: 0.1207 MSE valid loss: 0.2426\n",
      "Updating learning rate to 3.787878787878788e-05\n",
      "epoch: 66 MSE loss: 0.1206 MSE valid loss: 0.2418\n",
      "Updating learning rate to 3.759929927590882e-05\n",
      "epoch: 67 MSE loss: 0.1202 MSE valid loss: 0.2425\n",
      "Updating learning rate to 3.732590737770921e-05\n",
      "epoch: 68 MSE loss: 0.1201 MSE valid loss: 0.2429\n",
      "Updating learning rate to 3.7058393705829335e-05\n",
      "epoch: 69 MSE loss: 0.1201 MSE valid loss: 0.2421\n",
      "Updating learning rate to 3.679655058809149e-05\n",
      "epoch: 70 MSE loss: 0.1201 MSE valid loss: 0.2410\n",
      "Updating learning rate to 3.654018048087443e-05\n",
      "epoch: 71 MSE loss: 0.1189 MSE valid loss: 0.2416\n",
      "Updating learning rate to 3.6289095342709304e-05\n",
      "epoch: 72 MSE loss: 0.1192 MSE valid loss: 0.2460\n",
      "Updating learning rate to 3.604311605458291e-05\n",
      "epoch: 73 MSE loss: 0.1192 MSE valid loss: 0.2392\n",
      "Updating learning rate to 3.58020718828878e-05\n",
      "epoch: 74 MSE loss: 0.1186 MSE valid loss: 0.2419\n",
      "Updating learning rate to 3.5565799981359994e-05\n",
      "epoch: 75 MSE loss: 0.1193 MSE valid loss: 0.2440\n",
      "Updating learning rate to 3.5334144928703015e-05\n",
      "epoch: 76 MSE loss: 0.1187 MSE valid loss: 0.2422\n",
      "Updating learning rate to 3.510695829891526e-05\n",
      "epoch: 77 MSE loss: 0.1186 MSE valid loss: 0.2446\n",
      "Updating learning rate to 3.488409826162172e-05\n",
      "epoch: 78 MSE loss: 0.1181 MSE valid loss: 0.2438\n",
      "Updating learning rate to 3.4665429209964995e-05\n",
      "epoch: 79 MSE loss: 0.1180 MSE valid loss: 0.2419\n",
      "Updating learning rate to 3.445082141383733e-05\n",
      "epoch: 80 MSE loss: 0.1181 MSE valid loss: 0.2449\n",
      "Updating learning rate to 3.424015069643934e-05\n",
      "epoch: 81 MSE loss: 0.1181 MSE valid loss: 0.2453\n",
      "Updating learning rate to 3.4033298132333136e-05\n",
      "epoch: 82 MSE loss: 0.1176 MSE valid loss: 0.2446\n",
      "Updating learning rate to 3.383014976532195e-05\n",
      "epoch: 83 MSE loss: 0.1175 MSE valid loss: 0.2459\n",
      "Updating learning rate to 3.3630596344635906e-05\n",
      "epoch: 84 MSE loss: 0.1174 MSE valid loss: 0.2471\n",
      "Updating learning rate to 3.343453307803635e-05\n",
      "epoch: 85 MSE loss: 0.1173 MSE valid loss: 0.2444\n",
      "Updating learning rate to 3.3241859400571366e-05\n",
      "epoch: 86 MSE loss: 0.1175 MSE valid loss: 0.2497\n",
      "Updating learning rate to 3.305247875782325e-05\n",
      "epoch: 87 MSE loss: 0.1168 MSE valid loss: 0.2494\n",
      "Updating learning rate to 3.286629840258662e-05\n",
      "epoch: 88 MSE loss: 0.1165 MSE valid loss: 0.2438\n",
      "Updating learning rate to 3.268322920400467e-05\n",
      "epoch: 89 MSE loss: 0.1167 MSE valid loss: 0.2466\n",
      "Updating learning rate to 3.250318546827149e-05\n",
      "epoch: 90 MSE loss: 0.1165 MSE valid loss: 0.2458\n",
      "Updating learning rate to 3.232608477008077e-05\n",
      "epoch: 91 MSE loss: 0.1164 MSE valid loss: 0.2460\n",
      "Updating learning rate to 3.21518477940684e-05\n",
      "epoch: 92 MSE loss: 0.1170 MSE valid loss: 0.2457\n",
      "Updating learning rate to 3.198039818555568e-05\n",
      "epoch: 93 MSE loss: 0.1171 MSE valid loss: 0.2473\n",
      "Updating learning rate to 3.181166240995547e-05\n",
      "epoch: 94 MSE loss: 0.1159 MSE valid loss: 0.2468\n",
      "Updating learning rate to 3.1645569620253167e-05\n",
      "epoch: 95 MSE loss: 0.1161 MSE valid loss: 0.2444\n",
      "Updating learning rate to 3.148205153202001e-05\n",
      "epoch: 96 MSE loss: 0.1160 MSE valid loss: 0.2462\n",
      "Updating learning rate to 3.1321042305458135e-05\n",
      "epoch: 97 MSE loss: 0.1160 MSE valid loss: 0.2472\n",
      "Updating learning rate to 3.1162478434014266e-05\n",
      "epoch: 98 MSE loss: 0.1156 MSE valid loss: 0.2480\n",
      "Updating learning rate to 3.100629863913435e-05\n",
      "epoch: 99 MSE loss: 0.1154 MSE valid loss: 0.2478\n",
      "Updating learning rate to 4.811252243246882e-07\n",
      "Updating learning rate to 3.1754264805429416e-05\n",
      "epoch: 0 MSE loss: 1.5427 MSE valid loss: 1.3295\n",
      "Updating learning rate to 6.302740438653415e-05\n",
      "epoch: 1 MSE loss: 0.6343 MSE valid loss: 0.9283\n",
      "Updating learning rate to 9.430054396763888e-05\n",
      "epoch: 2 MSE loss: 0.3422 MSE valid loss: 0.5937\n",
      "Updating learning rate to 0.00012557368354874362\n",
      "epoch: 3 MSE loss: 0.2610 MSE valid loss: 0.4001\n",
      "Updating learning rate to 0.00013846219390542782\n",
      "epoch: 4 MSE loss: 0.2197 MSE valid loss: 0.3209\n",
      "Updating learning rate to 0.0001264304343560434\n",
      "epoch: 5 MSE loss: 0.1972 MSE valid loss: 0.3066\n",
      "Updating learning rate to 0.00011707322644771175\n",
      "epoch: 6 MSE loss: 0.1824 MSE valid loss: 0.3016\n",
      "Updating learning rate to 0.00010952698858458088\n",
      "epoch: 7 MSE loss: 0.1748 MSE valid loss: 0.2945\n",
      "Updating learning rate to 0.00010327404809650345\n",
      "epoch: 8 MSE loss: 0.1681 MSE valid loss: 0.2870\n",
      "Updating learning rate to 9.798272520870258e-05\n",
      "epoch: 9 MSE loss: 0.1629 MSE valid loss: 0.2853\n",
      "Updating learning rate to 9.342938659399198e-05\n",
      "epoch: 10 MSE loss: 0.1594 MSE valid loss: 0.2796\n",
      "Updating learning rate to 8.945703337056415e-05\n",
      "epoch: 11 MSE loss: 0.1561 MSE valid loss: 0.2780\n",
      "Updating learning rate to 8.595177052156611e-05\n",
      "epoch: 12 MSE loss: 0.1535 MSE valid loss: 0.2750\n",
      "Updating learning rate to 8.282869524032146e-05\n",
      "epoch: 13 MSE loss: 0.1504 MSE valid loss: 0.2728\n",
      "Updating learning rate to 8.002304995805999e-05\n",
      "epoch: 14 MSE loss: 0.1488 MSE valid loss: 0.2714\n",
      "Updating learning rate to 7.748446592171796e-05\n",
      "epoch: 15 MSE loss: 0.1472 MSE valid loss: 0.2701\n",
      "Updating learning rate to 7.517309741553296e-05\n",
      "epoch: 16 MSE loss: 0.1454 MSE valid loss: 0.2680\n",
      "Updating learning rate to 7.305695402335044e-05\n",
      "epoch: 17 MSE loss: 0.1443 MSE valid loss: 0.2680\n",
      "Updating learning rate to 7.111001549857179e-05\n",
      "epoch: 18 MSE loss: 0.1431 MSE valid loss: 0.2659\n",
      "Updating learning rate to 6.931087162517846e-05\n",
      "epoch: 19 MSE loss: 0.1431 MSE valid loss: 0.2659\n",
      "Updating learning rate to 6.76417225936176e-05\n",
      "epoch: 20 MSE loss: 0.1401 MSE valid loss: 0.2654\n",
      "Updating learning rate to 6.608763214317867e-05\n",
      "epoch: 21 MSE loss: 0.1394 MSE valid loss: 0.2677\n",
      "Updating learning rate to 6.46359612493774e-05\n",
      "epoch: 22 MSE loss: 0.1379 MSE valid loss: 0.2630\n",
      "Updating learning rate to 6.327593294406921e-05\n",
      "epoch: 23 MSE loss: 0.1382 MSE valid loss: 0.2613\n",
      "Updating learning rate to 6.199829383043036e-05\n",
      "epoch: 24 MSE loss: 0.1362 MSE valid loss: 0.2603\n",
      "Updating learning rate to 6.079504788572469e-05\n",
      "epoch: 25 MSE loss: 0.1348 MSE valid loss: 0.2638\n",
      "Updating learning rate to 5.965924498791846e-05\n",
      "epoch: 26 MSE loss: 0.1341 MSE valid loss: 0.2628\n",
      "Updating learning rate to 5.8584811349126495e-05\n",
      "epoch: 27 MSE loss: 0.1339 MSE valid loss: 0.2586\n",
      "Updating learning rate to 5.7566412382313e-05\n",
      "epoch: 28 MSE loss: 0.1329 MSE valid loss: 0.2595\n",
      "Updating learning rate to 5.659934091583233e-05\n",
      "epoch: 29 MSE loss: 0.1332 MSE valid loss: 0.2667\n",
      "Updating learning rate to 5.567942539842175e-05\n",
      "epoch: 30 MSE loss: 0.1313 MSE valid loss: 0.2619\n",
      "Updating learning rate to 5.4802954002676805e-05\n",
      "epoch: 31 MSE loss: 0.1307 MSE valid loss: 0.2625\n",
      "Updating learning rate to 5.39666114720432e-05\n",
      "epoch: 32 MSE loss: 0.1304 MSE valid loss: 0.2570\n",
      "Updating learning rate to 5.316742625738918e-05\n",
      "epoch: 33 MSE loss: 0.1298 MSE valid loss: 0.2587\n",
      "Updating learning rate to 5.240272601878982e-05\n",
      "epoch: 34 MSE loss: 0.1292 MSE valid loss: 0.2622\n",
      "Updating learning rate to 5.1670099971819504e-05\n",
      "epoch: 35 MSE loss: 0.1291 MSE valid loss: 0.2577\n",
      "Updating learning rate to 5.096736686795811e-05\n",
      "epoch: 36 MSE loss: 0.1284 MSE valid loss: 0.2669\n",
      "Updating learning rate to 5.0292547639160534e-05\n",
      "epoch: 37 MSE loss: 0.1275 MSE valid loss: 0.2649\n",
      "Updating learning rate to 4.96438419243461e-05\n",
      "epoch: 38 MSE loss: 0.1272 MSE valid loss: 0.2573\n",
      "Updating learning rate to 4.901960784313725e-05\n",
      "epoch: 39 MSE loss: 0.1265 MSE valid loss: 0.2618\n",
      "Updating learning rate to 4.841834449897069e-05\n",
      "epoch: 40 MSE loss: 0.1268 MSE valid loss: 0.2624\n",
      "Updating learning rate to 4.783867678672252e-05\n",
      "epoch: 41 MSE loss: 0.1259 MSE valid loss: 0.2697\n",
      "Updating learning rate to 4.727934215451461e-05\n",
      "epoch: 42 MSE loss: 0.1255 MSE valid loss: 0.2618\n",
      "Updating learning rate to 4.6739179029417445e-05\n",
      "epoch: 43 MSE loss: 0.1252 MSE valid loss: 0.2627\n",
      "Updating learning rate to 4.621711666540848e-05\n",
      "epoch: 44 MSE loss: 0.1248 MSE valid loss: 0.2625\n",
      "Updating learning rate to 4.571216621155238e-05\n",
      "epoch: 45 MSE loss: 0.1238 MSE valid loss: 0.2628\n",
      "Updating learning rate to 4.522341283077635e-05\n",
      "epoch: 46 MSE loss: 0.1236 MSE valid loss: 0.2559\n",
      "Updating learning rate to 4.4750008726252546e-05\n",
      "epoch: 47 MSE loss: 0.1234 MSE valid loss: 0.2672\n",
      "Updating learning rate to 4.4291166954394496e-05\n",
      "epoch: 48 MSE loss: 0.1231 MSE valid loss: 0.2639\n",
      "Updating learning rate to 4.384615592171157e-05\n",
      "epoch: 49 MSE loss: 0.1225 MSE valid loss: 0.2663\n",
      "Updating learning rate to 4.341429447794924e-05\n",
      "epoch: 50 MSE loss: 0.1223 MSE valid loss: 0.2648\n",
      "Updating learning rate to 4.299494753063186e-05\n",
      "epoch: 51 MSE loss: 0.1222 MSE valid loss: 0.2565\n",
      "Updating learning rate to 4.2587522116770666e-05\n",
      "epoch: 52 MSE loss: 0.1218 MSE valid loss: 0.2570\n",
      "Updating learning rate to 4.219146387646126e-05\n",
      "epoch: 53 MSE loss: 0.1212 MSE valid loss: 0.2620\n",
      "Updating learning rate to 4.1806253880665696e-05\n",
      "epoch: 54 MSE loss: 0.1214 MSE valid loss: 0.2718\n",
      "Updating learning rate to 4.143140577189121e-05\n",
      "epoch: 55 MSE loss: 0.1213 MSE valid loss: 0.2603\n",
      "Updating learning rate to 4.106646318193315e-05\n",
      "epoch: 56 MSE loss: 0.1214 MSE valid loss: 0.2603\n",
      "Updating learning rate to 4.071099739550263e-05\n",
      "epoch: 57 MSE loss: 0.1208 MSE valid loss: 0.2643\n",
      "Updating learning rate to 4.0364605232539164e-05\n",
      "epoch: 58 MSE loss: 0.1200 MSE valid loss: 0.2593\n",
      "Updating learning rate to 4.0026907125422175e-05\n",
      "epoch: 59 MSE loss: 0.1201 MSE valid loss: 0.2707\n",
      "Updating learning rate to 3.969754537023144e-05\n",
      "epoch: 60 MSE loss: 0.1197 MSE valid loss: 0.2645\n",
      "Updating learning rate to 3.937618253373847e-05\n",
      "epoch: 61 MSE loss: 0.1196 MSE valid loss: 0.2660\n",
      "Updating learning rate to 3.90625e-05\n",
      "epoch: 62 MSE loss: 0.1194 MSE valid loss: 0.2707\n",
      "Updating learning rate to 3.875619664232189e-05\n",
      "epoch: 63 MSE loss: 0.1195 MSE valid loss: 0.2710\n",
      "Updating learning rate to 3.845698760800996e-05\n",
      "epoch: 64 MSE loss: 0.1187 MSE valid loss: 0.2642\n",
      "Updating learning rate to 3.816460320475954e-05\n",
      "epoch: 65 MSE loss: 0.1194 MSE valid loss: 0.2742\n",
      "Updating learning rate to 3.787878787878788e-05\n",
      "epoch: 66 MSE loss: 0.1177 MSE valid loss: 0.2630\n",
      "Updating learning rate to 3.759929927590882e-05\n",
      "epoch: 67 MSE loss: 0.1175 MSE valid loss: 0.2682\n",
      "Updating learning rate to 3.732590737770921e-05\n",
      "epoch: 68 MSE loss: 0.1183 MSE valid loss: 0.2737\n",
      "Updating learning rate to 3.7058393705829335e-05\n",
      "epoch: 69 MSE loss: 0.1176 MSE valid loss: 0.2707\n",
      "Updating learning rate to 3.679655058809149e-05\n",
      "epoch: 70 MSE loss: 0.1172 MSE valid loss: 0.2664\n",
      "Updating learning rate to 3.654018048087443e-05\n",
      "epoch: 71 MSE loss: 0.1171 MSE valid loss: 0.2697\n",
      "Updating learning rate to 3.6289095342709304e-05\n",
      "epoch: 72 MSE loss: 0.1166 MSE valid loss: 0.2765\n",
      "Updating learning rate to 3.604311605458291e-05\n",
      "epoch: 73 MSE loss: 0.1168 MSE valid loss: 0.2649\n",
      "Updating learning rate to 3.58020718828878e-05\n",
      "epoch: 74 MSE loss: 0.1170 MSE valid loss: 0.2716\n",
      "Updating learning rate to 3.5565799981359994e-05\n",
      "epoch: 75 MSE loss: 0.1174 MSE valid loss: 0.2736\n",
      "Updating learning rate to 3.5334144928703015e-05\n",
      "epoch: 76 MSE loss: 0.1162 MSE valid loss: 0.2797\n",
      "Updating learning rate to 3.510695829891526e-05\n",
      "epoch: 77 MSE loss: 0.1161 MSE valid loss: 0.2796\n",
      "Updating learning rate to 3.488409826162172e-05\n",
      "epoch: 78 MSE loss: 0.1159 MSE valid loss: 0.2717\n",
      "Updating learning rate to 3.4665429209964995e-05\n",
      "epoch: 79 MSE loss: 0.1154 MSE valid loss: 0.2754\n",
      "Updating learning rate to 3.445082141383733e-05\n",
      "epoch: 80 MSE loss: 0.1153 MSE valid loss: 0.2686\n",
      "Updating learning rate to 3.424015069643934e-05\n",
      "epoch: 81 MSE loss: 0.1150 MSE valid loss: 0.2763\n",
      "Updating learning rate to 3.4033298132333136e-05\n",
      "epoch: 82 MSE loss: 0.1151 MSE valid loss: 0.2824\n",
      "Updating learning rate to 3.383014976532195e-05\n",
      "epoch: 83 MSE loss: 0.1152 MSE valid loss: 0.2815\n",
      "Updating learning rate to 3.3630596344635906e-05\n",
      "epoch: 84 MSE loss: 0.1144 MSE valid loss: 0.2796\n",
      "Updating learning rate to 3.343453307803635e-05\n",
      "epoch: 85 MSE loss: 0.1146 MSE valid loss: 0.2771\n",
      "Updating learning rate to 3.3241859400571366e-05\n",
      "epoch: 86 MSE loss: 0.1144 MSE valid loss: 0.2777\n",
      "Updating learning rate to 3.305247875782325e-05\n",
      "epoch: 87 MSE loss: 0.1141 MSE valid loss: 0.2770\n",
      "Updating learning rate to 3.286629840258662e-05\n",
      "epoch: 88 MSE loss: 0.1135 MSE valid loss: 0.2754\n",
      "Updating learning rate to 3.268322920400467e-05\n",
      "epoch: 89 MSE loss: 0.1141 MSE valid loss: 0.2795\n",
      "Updating learning rate to 3.250318546827149e-05\n",
      "epoch: 90 MSE loss: 0.1140 MSE valid loss: 0.2770\n",
      "Updating learning rate to 3.232608477008077e-05\n",
      "epoch: 91 MSE loss: 0.1139 MSE valid loss: 0.2747\n",
      "Updating learning rate to 3.21518477940684e-05\n",
      "epoch: 92 MSE loss: 0.1130 MSE valid loss: 0.2760\n",
      "Updating learning rate to 3.198039818555568e-05\n",
      "epoch: 93 MSE loss: 0.1129 MSE valid loss: 0.2851\n",
      "Updating learning rate to 3.181166240995547e-05\n",
      "epoch: 94 MSE loss: 0.1138 MSE valid loss: 0.2789\n",
      "Updating learning rate to 3.1645569620253167e-05\n",
      "epoch: 95 MSE loss: 0.1132 MSE valid loss: 0.2873\n",
      "Updating learning rate to 3.148205153202001e-05\n",
      "epoch: 96 MSE loss: 0.1129 MSE valid loss: 0.2753\n",
      "Updating learning rate to 3.1321042305458135e-05\n",
      "epoch: 97 MSE loss: 0.1124 MSE valid loss: 0.2785\n",
      "Updating learning rate to 3.1162478434014266e-05\n",
      "epoch: 98 MSE loss: 0.1125 MSE valid loss: 0.2823\n",
      "Updating learning rate to 3.100629863913435e-05\n",
      "epoch: 99 MSE loss: 0.1129 MSE valid loss: 0.2916\n",
      "Updating learning rate to 4.811252243246882e-08\n",
      "Updating learning rate to 3.175426480542942e-06\n",
      "epoch: 0 MSE loss: 0.2407 MSE valid loss: 0.1341\n",
      "Updating learning rate to 6.302740438653415e-06\n",
      "epoch: 1 MSE loss: 0.2364 MSE valid loss: 0.1293\n",
      "Updating learning rate to 9.430054396763887e-06\n",
      "epoch: 2 MSE loss: 0.2266 MSE valid loss: 0.1227\n",
      "Updating learning rate to 1.2557368354874362e-05\n",
      "epoch: 3 MSE loss: 0.2167 MSE valid loss: 0.1168\n",
      "Updating learning rate to 1.3846219390542782e-05\n",
      "epoch: 4 MSE loss: 0.2047 MSE valid loss: 0.1119\n",
      "Updating learning rate to 1.264304343560434e-05\n",
      "epoch: 5 MSE loss: 0.1955 MSE valid loss: 0.1084\n",
      "Updating learning rate to 1.1707322644771175e-05\n",
      "epoch: 6 MSE loss: 0.1873 MSE valid loss: 0.1065\n",
      "Updating learning rate to 1.0952698858458087e-05\n",
      "epoch: 7 MSE loss: 0.1820 MSE valid loss: 0.1052\n",
      "Updating learning rate to 1.0327404809650346e-05\n",
      "epoch: 8 MSE loss: 0.1772 MSE valid loss: 0.1047\n",
      "Updating learning rate to 9.798272520870257e-06\n",
      "epoch: 9 MSE loss: 0.1726 MSE valid loss: 0.1046\n",
      "Updating learning rate to 9.342938659399199e-06\n",
      "epoch: 10 MSE loss: 0.1686 MSE valid loss: 0.1044\n",
      "Updating learning rate to 8.945703337056415e-06\n",
      "epoch: 11 MSE loss: 0.1656 MSE valid loss: 0.1046\n",
      "Updating learning rate to 8.595177052156612e-06\n",
      "epoch: 12 MSE loss: 0.1629 MSE valid loss: 0.1048\n",
      "Updating learning rate to 8.282869524032146e-06\n",
      "epoch: 13 MSE loss: 0.1603 MSE valid loss: 0.1051\n",
      "Updating learning rate to 8.002304995806e-06\n",
      "epoch: 14 MSE loss: 0.1591 MSE valid loss: 0.1057\n",
      "Updating learning rate to 7.748446592171797e-06\n",
      "epoch: 15 MSE loss: 0.1565 MSE valid loss: 0.1061\n",
      "Updating learning rate to 7.517309741553296e-06\n",
      "epoch: 16 MSE loss: 0.1552 MSE valid loss: 0.1068\n",
      "Updating learning rate to 7.305695402335044e-06\n",
      "epoch: 17 MSE loss: 0.1537 MSE valid loss: 0.1073\n",
      "Updating learning rate to 7.1110015498571785e-06\n",
      "epoch: 18 MSE loss: 0.1515 MSE valid loss: 0.1077\n",
      "Updating learning rate to 6.931087162517846e-06\n",
      "epoch: 19 MSE loss: 0.1516 MSE valid loss: 0.1086\n",
      "Updating learning rate to 6.76417225936176e-06\n",
      "epoch: 20 MSE loss: 0.1490 MSE valid loss: 0.1088\n",
      "Updating learning rate to 6.608763214317868e-06\n",
      "epoch: 21 MSE loss: 0.1480 MSE valid loss: 0.1093\n",
      "Updating learning rate to 6.463596124937739e-06\n",
      "epoch: 22 MSE loss: 0.1475 MSE valid loss: 0.1094\n",
      "Updating learning rate to 6.327593294406922e-06\n",
      "epoch: 23 MSE loss: 0.1465 MSE valid loss: 0.1102\n",
      "Updating learning rate to 6.199829383043036e-06\n",
      "epoch: 24 MSE loss: 0.1451 MSE valid loss: 0.1105\n",
      "Updating learning rate to 6.079504788572469e-06\n",
      "epoch: 25 MSE loss: 0.1444 MSE valid loss: 0.1112\n",
      "Updating learning rate to 5.965924498791846e-06\n",
      "epoch: 26 MSE loss: 0.1428 MSE valid loss: 0.1115\n",
      "Updating learning rate to 5.8584811349126495e-06\n",
      "epoch: 27 MSE loss: 0.1429 MSE valid loss: 0.1120\n",
      "Updating learning rate to 5.7566412382313e-06\n",
      "epoch: 28 MSE loss: 0.1416 MSE valid loss: 0.1126\n",
      "Updating learning rate to 5.659934091583234e-06\n",
      "epoch: 29 MSE loss: 0.1412 MSE valid loss: 0.1129\n",
      "Updating learning rate to 5.567942539842175e-06\n",
      "epoch: 30 MSE loss: 0.1400 MSE valid loss: 0.1132\n",
      "Updating learning rate to 5.48029540026768e-06\n",
      "epoch: 31 MSE loss: 0.1403 MSE valid loss: 0.1134\n",
      "Updating learning rate to 5.39666114720432e-06\n",
      "epoch: 32 MSE loss: 0.1395 MSE valid loss: 0.1137\n",
      "Updating learning rate to 5.316742625738919e-06\n",
      "epoch: 33 MSE loss: 0.1384 MSE valid loss: 0.1142\n",
      "Updating learning rate to 5.240272601878983e-06\n",
      "epoch: 34 MSE loss: 0.1371 MSE valid loss: 0.1140\n",
      "Updating learning rate to 5.16700999718195e-06\n",
      "epoch: 35 MSE loss: 0.1376 MSE valid loss: 0.1145\n",
      "Updating learning rate to 5.096736686795811e-06\n",
      "epoch: 36 MSE loss: 0.1370 MSE valid loss: 0.1151\n",
      "Updating learning rate to 5.029254763916053e-06\n",
      "epoch: 37 MSE loss: 0.1355 MSE valid loss: 0.1151\n",
      "Updating learning rate to 4.964384192434611e-06\n",
      "epoch: 38 MSE loss: 0.1353 MSE valid loss: 0.1156\n",
      "Updating learning rate to 4.901960784313726e-06\n",
      "epoch: 39 MSE loss: 0.1346 MSE valid loss: 0.1163\n",
      "Updating learning rate to 4.841834449897069e-06\n",
      "epoch: 40 MSE loss: 0.1347 MSE valid loss: 0.1165\n",
      "Updating learning rate to 4.783867678672252e-06\n",
      "epoch: 41 MSE loss: 0.1337 MSE valid loss: 0.1163\n",
      "Updating learning rate to 4.727934215451461e-06\n",
      "epoch: 42 MSE loss: 0.1335 MSE valid loss: 0.1167\n",
      "Updating learning rate to 4.673917902941745e-06\n",
      "epoch: 43 MSE loss: 0.1324 MSE valid loss: 0.1166\n",
      "Updating learning rate to 4.621711666540848e-06\n",
      "epoch: 44 MSE loss: 0.1326 MSE valid loss: 0.1169\n",
      "Updating learning rate to 4.571216621155238e-06\n",
      "epoch: 45 MSE loss: 0.1316 MSE valid loss: 0.1172\n",
      "Updating learning rate to 4.5223412830776355e-06\n",
      "epoch: 46 MSE loss: 0.1313 MSE valid loss: 0.1174\n",
      "Updating learning rate to 4.475000872625255e-06\n",
      "epoch: 47 MSE loss: 0.1312 MSE valid loss: 0.1176\n",
      "Updating learning rate to 4.42911669543945e-06\n",
      "epoch: 48 MSE loss: 0.1302 MSE valid loss: 0.1181\n",
      "Updating learning rate to 4.3846155921711576e-06\n",
      "epoch: 49 MSE loss: 0.1304 MSE valid loss: 0.1181\n",
      "Updating learning rate to 4.341429447794925e-06\n",
      "epoch: 50 MSE loss: 0.1301 MSE valid loss: 0.1183\n",
      "Updating learning rate to 4.299494753063186e-06\n",
      "epoch: 51 MSE loss: 0.1291 MSE valid loss: 0.1180\n",
      "Updating learning rate to 4.258752211677067e-06\n",
      "epoch: 52 MSE loss: 0.1296 MSE valid loss: 0.1181\n",
      "Updating learning rate to 4.219146387646126e-06\n",
      "epoch: 53 MSE loss: 0.1290 MSE valid loss: 0.1187\n",
      "Updating learning rate to 4.18062538806657e-06\n",
      "epoch: 54 MSE loss: 0.1280 MSE valid loss: 0.1190\n",
      "Updating learning rate to 4.143140577189122e-06\n",
      "epoch: 55 MSE loss: 0.1282 MSE valid loss: 0.1198\n",
      "Updating learning rate to 4.106646318193315e-06\n",
      "epoch: 56 MSE loss: 0.1273 MSE valid loss: 0.1194\n",
      "Updating learning rate to 4.071099739550263e-06\n",
      "epoch: 57 MSE loss: 0.1269 MSE valid loss: 0.1194\n",
      "Updating learning rate to 4.036460523253916e-06\n",
      "epoch: 58 MSE loss: 0.1268 MSE valid loss: 0.1195\n",
      "Updating learning rate to 4.002690712542218e-06\n",
      "epoch: 59 MSE loss: 0.1266 MSE valid loss: 0.1196\n",
      "Updating learning rate to 3.969754537023144e-06\n",
      "epoch: 60 MSE loss: 0.1268 MSE valid loss: 0.1204\n",
      "Updating learning rate to 3.937618253373847e-06\n",
      "epoch: 61 MSE loss: 0.1260 MSE valid loss: 0.1203\n",
      "Updating learning rate to 3.90625e-06\n",
      "epoch: 62 MSE loss: 0.1249 MSE valid loss: 0.1204\n",
      "Updating learning rate to 3.8756196642321895e-06\n",
      "epoch: 63 MSE loss: 0.1254 MSE valid loss: 0.1204\n",
      "Updating learning rate to 3.845698760800996e-06\n",
      "epoch: 64 MSE loss: 0.1250 MSE valid loss: 0.1205\n",
      "Updating learning rate to 3.816460320475954e-06\n",
      "epoch: 65 MSE loss: 0.1251 MSE valid loss: 0.1207\n",
      "Updating learning rate to 3.7878787878787882e-06\n",
      "epoch: 66 MSE loss: 0.1247 MSE valid loss: 0.1209\n",
      "Updating learning rate to 3.759929927590882e-06\n",
      "epoch: 67 MSE loss: 0.1234 MSE valid loss: 0.1212\n",
      "Updating learning rate to 3.732590737770921e-06\n",
      "epoch: 68 MSE loss: 0.1234 MSE valid loss: 0.1213\n",
      "Updating learning rate to 3.7058393705829336e-06\n",
      "epoch: 69 MSE loss: 0.1234 MSE valid loss: 0.1214\n",
      "Updating learning rate to 3.6796550588091485e-06\n",
      "epoch: 70 MSE loss: 0.1239 MSE valid loss: 0.1218\n",
      "Updating learning rate to 3.6540180480874438e-06\n",
      "epoch: 71 MSE loss: 0.1229 MSE valid loss: 0.1218\n",
      "Updating learning rate to 3.6289095342709304e-06\n",
      "epoch: 72 MSE loss: 0.1234 MSE valid loss: 0.1217\n",
      "Updating learning rate to 3.604311605458291e-06\n",
      "epoch: 73 MSE loss: 0.1231 MSE valid loss: 0.1221\n",
      "Updating learning rate to 3.58020718828878e-06\n",
      "epoch: 74 MSE loss: 0.1219 MSE valid loss: 0.1225\n",
      "Updating learning rate to 3.5565799981359997e-06\n",
      "epoch: 75 MSE loss: 0.1218 MSE valid loss: 0.1222\n",
      "Updating learning rate to 3.5334144928703013e-06\n",
      "epoch: 76 MSE loss: 0.1230 MSE valid loss: 0.1221\n",
      "Updating learning rate to 3.5106958298915256e-06\n",
      "epoch: 77 MSE loss: 0.1223 MSE valid loss: 0.1223\n",
      "Updating learning rate to 3.4884098261621723e-06\n",
      "epoch: 78 MSE loss: 0.1210 MSE valid loss: 0.1230\n",
      "Updating learning rate to 3.466542920996499e-06\n",
      "epoch: 79 MSE loss: 0.1207 MSE valid loss: 0.1228\n",
      "Updating learning rate to 3.4450821413837328e-06\n",
      "epoch: 80 MSE loss: 0.1215 MSE valid loss: 0.1233\n",
      "Updating learning rate to 3.4240150696439336e-06\n",
      "epoch: 81 MSE loss: 0.1209 MSE valid loss: 0.1233\n",
      "Updating learning rate to 3.4033298132333132e-06\n",
      "epoch: 82 MSE loss: 0.1200 MSE valid loss: 0.1237\n",
      "Updating learning rate to 3.383014976532195e-06\n",
      "epoch: 83 MSE loss: 0.1201 MSE valid loss: 0.1234\n",
      "Updating learning rate to 3.3630596344635904e-06\n",
      "epoch: 84 MSE loss: 0.1200 MSE valid loss: 0.1239\n",
      "Updating learning rate to 3.3434533078036348e-06\n",
      "epoch: 85 MSE loss: 0.1211 MSE valid loss: 0.1243\n",
      "Updating learning rate to 3.3241859400571367e-06\n",
      "epoch: 86 MSE loss: 0.1195 MSE valid loss: 0.1242\n",
      "Updating learning rate to 3.305247875782325e-06\n",
      "epoch: 87 MSE loss: 0.1189 MSE valid loss: 0.1243\n",
      "Updating learning rate to 3.2866298402586612e-06\n",
      "epoch: 88 MSE loss: 0.1208 MSE valid loss: 0.1247\n",
      "Updating learning rate to 3.2683229204004673e-06\n",
      "epoch: 89 MSE loss: 0.1184 MSE valid loss: 0.1242\n",
      "Updating learning rate to 3.2503185468271486e-06\n",
      "epoch: 90 MSE loss: 0.1184 MSE valid loss: 0.1250\n",
      "Updating learning rate to 3.232608477008077e-06\n",
      "epoch: 91 MSE loss: 0.1187 MSE valid loss: 0.1244\n",
      "Updating learning rate to 3.2151847794068398e-06\n",
      "epoch: 92 MSE loss: 0.1180 MSE valid loss: 0.1253\n",
      "Updating learning rate to 3.1980398185555676e-06\n",
      "epoch: 93 MSE loss: 0.1188 MSE valid loss: 0.1253\n",
      "Updating learning rate to 3.1811662409955475e-06\n",
      "epoch: 94 MSE loss: 0.1178 MSE valid loss: 0.1251\n",
      "Updating learning rate to 3.1645569620253167e-06\n",
      "epoch: 95 MSE loss: 0.1175 MSE valid loss: 0.1252\n",
      "Updating learning rate to 3.1482051532020014e-06\n",
      "epoch: 96 MSE loss: 0.1182 MSE valid loss: 0.1260\n",
      "Updating learning rate to 3.1321042305458136e-06\n",
      "epoch: 97 MSE loss: 0.1172 MSE valid loss: 0.1256\n",
      "Updating learning rate to 3.116247843401426e-06\n",
      "epoch: 98 MSE loss: 0.1177 MSE valid loss: 0.1260\n",
      "Updating learning rate to 3.1006298639134353e-06\n",
      "epoch: 99 MSE loss: 0.1172 MSE valid loss: 0.1264\n",
      "Elapsed Time: 88.645033785142\n"
     ]
    }
   ],
   "source": [
    "c_in = 7\n",
    "context_window = 336\n",
    "target_window = 96\n",
    "patch_len = 16\n",
    "stride = 8 \n",
    "\n",
    "patchtst_model = PatchTST(c_in=c_in, context_window=context_window, target_window=target_window, patch_len=patch_len, stride=stride)\n",
    "\n",
    "Linear_model = Linear(c_in=c_in, context_window=context_window, target_window=target_window)\n",
    "DLinear_model = DLinear(c_in=c_in, context_window=context_window, target_window=target_window)\n",
    "\n",
    "Linear_learner = Learner(model=Linear_model, dataset=ETTDataset, adjust_lr=True, adjust_factor=0.01)\n",
    "DLinear_learner = Learner(model=DLinear_model, dataset=ETTDataset, adjust_lr=True, adjust_factor=0.01)\n",
    "patchtst_learner = Learner(model=patchtst_model, dataset=ETTDataset, adjust_lr=True, adjust_factor=0.001)\n",
    "\n",
    "Linear_train_history, Linear_valid_history = Linear_learner.train()\n",
    "DLinear_train_history, DLinear_valid_history = DLinear_learner.train()\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "patchtst_train_history, patchtst_valid_history = patchtst_learner.train()\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(\"Elapsed Time:\", elapsed_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "ax.plot(np.arange(1, Linear_learner.epochs+1), Linear_valid_history, label=\"Linear\")\n",
    "ax.plot(np.arange(1, DLinear_learner.epochs+1), DLinear_valid_history, label=\"DLinear\")\n",
    "ax.plot(np.arange(1, patchtst_learner.epochs+1), patchtst_valid_history, label=\"PatchTST\")\n",
    "ax.set_xlabel(\"epochs\")\n",
    "ax.set_ylabel(\"MSE Error\")\n",
    "ax.set_ylim(0, 1.5)\n",
    "ax.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 1.789MB\n"
     ]
    }
   ],
   "source": [
    "model = patchtst_model\n",
    "param_size = 0\n",
    "for param in model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('model size: {:.3f}MB'.format(size_all_mb))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
